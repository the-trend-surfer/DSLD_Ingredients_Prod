{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Supplement Ingredient Evidence Collector\n",
    "\n",
    "**–ü—Ä–æ—î–∫—Ç:** –ê–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–∏–π –∑–±—ñ—Ä –Ω–∞—É–∫–æ–≤–∏—Ö –¥–æ–∫–∞–∑—ñ–≤ –¥–ª—è —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤ —Ö–∞—Ä—á–æ–≤–∏—Ö –¥–æ–±–∞–≤–æ–∫\n",
    "**Version:** v1.0\n",
    "**Date:** 2025-09-19\n",
    "\n",
    "---\n",
    "\n",
    "## –û–ø–∏—Å\n",
    "–°–∏—Å—Ç–µ–º–∞ –¥–ª—è –º–∞—Å–æ–≤–æ—ó –æ–±—Ä–æ–±–∫–∏ 6500+ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤ –∑ Google Sheets —Ç–∞ –∑–±–æ—Ä—É —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö –ø—Ä–æ –ø–æ—Ö–æ–¥–∂–µ–Ω–Ω—è, –∞–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏ —Ç–∞ –¥–æ–±–æ–≤—ñ –Ω–æ—Ä–º–∏ –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é —Ü–∏—Ç–∞—Ç.\n",
    "\n",
    "## –°—Ç—Ä—É–∫—Ç—É—Ä–∞\n",
    "- **Cell 0:** Config —Ç–∞ –∞–≤—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è\n",
    "- **Cell 1:** Google Sheets API\n",
    "- **Cell 2:** OpenAI API —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è\n",
    "- **Cell 3:** –ü–æ—à—É–∫ —Ç–∞ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—è\n",
    "- **Cell 4:** Batch –æ–±—Ä–æ–±–∫–∞\n",
    "- **Cell 5:** –ï–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
    "- **Cell 6:** Quality assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cell_0_config"
   },
   "outputs": [],
   "source": [
    "#@title Cell 0 ‚Äî Configuration and Authentication\n",
    "#@markdown –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —Ç–∞ –∞–≤—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# === CORE CONFIG ===\n",
    "PROJECT_NAME = \"DLSD Evidence Collector\"\n",
    "VERSION = \"v1.0\"\n",
    "RUN_ID = f\"run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"üöÄ {PROJECT_NAME} {VERSION}\")\n",
    "print(f\"üìä Run ID: {RUN_ID}\")\n",
    "print(f\"‚è∞ Started: {datetime.datetime.now().isoformat()}\")\n",
    "\n",
    "# === GOOGLE SHEETS CONFIG ===\n",
    "# –î–∞–Ω—ñ –∑ 01_data_sources.md\n",
    "SHEET_ID_ING = \"1kOrSOPgn7IDdA170YJDRBQw4Wt2-Y8uX0PdvCfxY4qA\"\n",
    "SHEET_NAME_ING = \"Ingredients_Main\"\n",
    "RANGE_INGREDIENTS = \"C2:C\"  # –æ—Å–Ω–æ–≤–Ω—ñ —ñ–º–µ–Ω–∞ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤\n",
    "RANGE_SYNONYMS_EN_LAT = \"E2:E\"  # –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ/–ª–∞—Ç–∏–Ω—Å—å–∫—ñ —Å–∏–Ω–æ–Ω—ñ–º–∏\n",
    "\n",
    "# === OPENAI CONFIG ===\n",
    "# TODO: –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ —á–µ—Ä–µ–∑ Colab Secrets\n",
    "OPENAI_MODEL = \"gpt-4o\"  # –∞–±–æ gpt-4o-mini –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è\n",
    "MAX_TOKENS = 4096\n",
    "TEMPERATURE = 0.1  # –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç—ñ\n",
    "\n",
    "# === D–û–ó–í–û–õ–ï–ù–Ü –î–û–ú–ï–ù–ò ===\n",
    "# –ó 02_project_constraints.md\n",
    "ALLOWED_DOMAINS = [\n",
    "    \"nih.gov\", \"ncbi.nlm.nih.gov\", \"efsa.europa.eu\",\n",
    "    \"examine.com\", \"consumerlab.com\", \"sciencedirect.com\", \"nature.com\"\n",
    "]\n",
    "\n",
    "# === –†–û–ë–û–ß–Ü –î–ò–†–ï–ö–¢–û–†–Ü–á ===\n",
    "WORK_DIR = \"/content/dlsd_work\"\n",
    "RESULTS_DIR = f\"{WORK_DIR}/results\"\n",
    "LOGS_DIR = f\"{WORK_DIR}/logs\"\n",
    "\n",
    "# –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Working directory: {WORK_DIR}\")\n",
    "print(f\"üíæ Results will be saved to: {RESULTS_DIR}\")\n",
    "print(\"‚úÖ Configuration completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cell_1_sheets_api"
   },
   "outputs": [],
   "source": [
    "#@title Cell 1 ‚Äî Google Sheets API Setup\n",
    "#@markdown –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Sheets —Ç–∞ —á–∏—Ç–∞–Ω–Ω—è —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤\n",
    "\n",
    "# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –±—ñ–±–ª—ñ–æ—Ç–µ–∫\n",
    "!pip install -q gspread google-auth google-auth-oauthlib google-auth-httplib2\n",
    "\n",
    "import gspread\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.colab import auth, files\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === –ê–í–¢–ï–ù–¢–ò–§–Ü–ö–ê–¶–Ü–Ø ===\n",
    "# –í–∞—Ä—ñ–∞–Ω—Ç 1: –ß–µ—Ä–µ–∑ Google Colab auth (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è)\n",
    "auth.authenticate_user()\n",
    "import gspread\n",
    "from google.auth import default\n",
    "creds, _ = default()\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "# –í–∞—Ä—ñ–∞–Ω—Ç 2: Service Account (–¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É)\n",
    "# credentials_path = \"/content/service_account.json\"  # –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª\n",
    "# scope = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "# creds = Credentials.from_service_account_file(credentials_path, scopes=scope)\n",
    "# gc = gspread.authorize(creds)\n",
    "\n",
    "print(\"üîë Google Sheets API authenticated\")\n",
    "\n",
    "# === –ß–ò–¢–ê–ù–ù–Ø –î–ê–ù–ò–• ===\n",
    "def load_ingredients_from_sheets():\n",
    "    \"\"\"–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∏ —Ç–∞ —Å–∏–Ω–æ–Ω—ñ–º–∏ –∑ Google Sheets\"\"\"\n",
    "    try:\n",
    "        # –í—ñ–¥–∫—Ä–∏—Ç–∏ —Ç–∞–±–ª–∏—Ü—é\n",
    "        sheet = gc.open_by_key(SHEET_ID_ING).worksheet(SHEET_NAME_ING)\n",
    "        print(f\"üìã Opened sheet: {SHEET_NAME_ING}\")\n",
    "        \n",
    "        # –ß–∏—Ç–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–∏—Ö —ñ–º–µ–Ω (C2:C)\n",
    "        ingredients_raw = sheet.get(RANGE_INGREDIENTS)\n",
    "        ingredients = [row[0] for row in ingredients_raw if row and row[0].strip()]\n",
    "        \n",
    "        # –ß–∏—Ç–∞–Ω–Ω—è —Å–∏–Ω–æ–Ω—ñ–º—ñ–≤ (E2:E)\n",
    "        synonyms_raw = sheet.get(RANGE_SYNONYMS_EN_LAT)\n",
    "        synonyms = [row[0] for row in synonyms_raw if row and row[0].strip()]\n",
    "        \n",
    "        print(f\"üìä Loaded {len(ingredients)} ingredients\")\n",
    "        print(f\"üìä Loaded {len(synonyms)} synonym entries\")\n",
    "        \n",
    "        return ingredients, synonyms\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def build_query_terms(ingredients, synonyms):\n",
    "    \"\"\"–§–æ—Ä–º—É—î —Å–ø–∏—Å–æ–∫ query terms –∑ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—î—é —Ç–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é\"\"\"\n",
    "    raw_terms = []\n",
    "    \n",
    "    # –î–æ–¥–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω—ñ —ñ–º–µ–Ω–∞\n",
    "    for ingredient in ingredients:\n",
    "        raw_terms.append(ingredient.strip())\n",
    "    \n",
    "    # –î–æ–¥–∞—Ç–∏ —Å–∏–Ω–æ–Ω—ñ–º–∏ (—Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –∫–æ–º–∞–º–∏)\n",
    "    for synonym_entry in synonyms:\n",
    "        for synonym in re.split(r\",|;|\\n\", str(synonym_entry)):\n",
    "            synonym = synonym.strip()\n",
    "            if synonym:\n",
    "                raw_terms.append(synonym)\n",
    "    \n",
    "    # –î–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è —Ç–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è\n",
    "    query_terms = []\n",
    "    seen = set()\n",
    "    \n",
    "    for term in raw_terms:\n",
    "        key = term.lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —ñ–∑–æ–ª—å–æ–≤–∞–Ω—ñ –∞–±—Ä–µ–≤—ñ–∞—Ç—É—Ä–∏\n",
    "            if len(term) <= 5 and term.upper() == term:\n",
    "                continue\n",
    "            query_terms.append(term)\n",
    "    \n",
    "    print(f\"üîç Built {len(query_terms)} unique query terms\")\n",
    "    return query_terms\n",
    "\n",
    "# === –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ê–ù–ò–• ===\n",
    "ingredients_list, synonyms_list = load_ingredients_from_sheets()\n",
    "query_terms = build_query_terms(ingredients_list, synonyms_list)\n",
    "\n",
    "# –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–µ—Ä—à—ñ –∫—ñ–ª—å–∫–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤\n",
    "if query_terms:\n",
    "    print(\"\\nüìù Sample query terms:\")\n",
    "    for i, term in enumerate(query_terms[:10]):\n",
    "        print(f\"  {i+1}. {term}\")\n",
    "    if len(query_terms) > 10:\n",
    "        print(f\"  ... and {len(query_terms) - 10} more\")\n",
    "\n",
    "print(\"\\n‚úÖ Google Sheets data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cell_2_openai_api"
   },
   "outputs": [],
   "source": "#@title Cell 2 ‚Äî OpenAI API Integration\n#@markdown –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è OpenAI API –∑ JSON Schema –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é\n\n# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –±—ñ–±–ª—ñ–æ—Ç–µ–∫\n!pip install -q openai jsonschema\n\nimport openai\nimport jsonschema\nfrom google.colab import userdata\nimport json\n\n# === OPENAI SETUP ===\n# –û—Ç—Ä–∏–º–∞—Ç–∏ API –∫–ª—é—á –∑ Colab Secrets\ntry:\n    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n    print(\"üîë OpenAI API key loaded from secrets\")\nexcept Exception as e:\n    print(f\"‚ùå Please add OPENAI_API_KEY to Colab secrets: {e}\")\n    # Fallback: manual input\n    # OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API Key: \")\n    # client = openai.OpenAI(api_key=OPENAI_API_KEY)\n\n# === JSON SCHEMA ===\n# –°—Ö–µ–º–∞ –∑ 04_system_prompt_json_schema.md\nEVIDENCE_SCHEMA = {\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"$id\": \"https://example.org/schemas/supplement_evidence_v1.json\",\n    \"title\": \"Supplement Ingredient Evidence\",\n    \"type\": \"object\",\n    \"required\": [\n        \"ingredient_name_uk\",\n        \"ingredient_name_lat\",\n        \"result_status\",\n        \"sources\",\n        \"citations\",\n        \"provenance\"\n    ],\n    \"properties\": {\n        \"ingredient_name_uk\": {\"type\": \"string\", \"minLength\": 1},\n        \"ingredient_name_lat\": {\"type\": \"string\", \"minLength\": 1},\n        \"synonyms\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n        \"source_material\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"kingdom\": {\n                    \"type\": [\"string\", \"null\"],\n                    \"enum\": [\"–†–æ—Å–ª–∏–Ω–∏\", \"–¢–≤–∞—Ä–∏–Ω–∏\", \"–ì—Ä–∏–±–∏\", \"–ü—Ä–æ—Ç–∏—Å—Ç–∏\", \"–ë–∞–∫—Ç–µ—Ä—ñ—ó\", \"–ê—Ä—Ö–µ—ó\", \"–ú—ñ–Ω–µ—Ä–∞–ª–∏\", \"–°–∏–Ω—Ç–µ—Ç–∏—á–Ω–µ\", None]\n                },\n                \"part_or_origin\": {\"type\": [\"string\", \"null\"]},\n                \"description\": {\"type\": [\"string\", \"null\"]}\n            },\n            \"required\": [\"kingdom\"],\n            \"additionalProperties\": False\n        },\n        \"active_compounds\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"name\"],\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"iupac\": {\"type\": [\"string\", \"null\"]},\n                    \"cas\": {\"type\": [\"string\", \"null\"]},\n                    \"concentration\": {\n                        \"type\": [\"object\", \"null\"],\n                        \"properties\": {\n                            \"value\": {\"type\": [\"number\", \"null\"]},\n                            \"min_value\": {\"type\": [\"number\", \"null\"]},\n                            \"max_value\": {\"type\": [\"number\", \"null\"]},\n                            \"unit\": {\n                                \"type\": [\"string\", \"null\"],\n                                \"enum\": [\"%\", \"–º–≥/–≥\", \"–º–∫–≥/–≥\", \"–º–≥/–º–ª\", \"–º–∫–≥/–º–ª\", \"ppm\", None]\n                            }\n                        },\n                        \"additionalProperties\": False\n                    }\n                },\n                \"additionalProperties\": False\n            }\n        },\n        \"daily_dose\": {\n            \"type\": [\"object\", \"null\"],\n            \"properties\": {\n                \"recommended\": {\n                    \"type\": [\"object\", \"null\"],\n                    \"properties\": {\n                        \"value\": {\"type\": [\"number\", \"null\"]},\n                        \"min_value\": {\"type\": [\"number\", \"null\"]},\n                        \"max_value\": {\"type\": [\"number\", \"null\"]},\n                        \"unit\": {\n                            \"type\": [\"string\", \"null\"],\n                            \"enum\": [\"–º–≥/–¥–µ–Ω—å\", \"–≥/–¥–µ–Ω—å\", \"–º–∫–≥/–¥–µ–Ω—å\", \"–ú–û/–¥–µ–Ω—å\", \"–º–≥/–∫–≥/–¥–µ–Ω—å\", None]\n                        }\n                    },\n                    \"additionalProperties\": False\n                },\n                \"upper_limit\": {\n                    \"type\": [\"object\", \"null\"],\n                    \"properties\": {\n                        \"value\": {\"type\": [\"number\", \"null\"]},\n                        \"min_value\": {\"type\": [\"number\", \"null\"]},\n                        \"max_value\": {\"type\": [\"number\", \"null\"]},\n                        \"unit\": {\n                            \"type\": [\"string\", \"null\"],\n                            \"enum\": [\"–º–≥/–¥–µ–Ω—å\", \"–≥/–¥–µ–Ω—å\", \"–º–∫–≥/–¥–µ–Ω—å\", \"–ú–û/–¥–µ–Ω—å\", \"–º–≥/–∫–≥/–¥–µ–Ω—å\", None]\n                        }\n                    },\n                    \"additionalProperties\": False\n                },\n                \"evidence_type\": {\n                    \"type\": [\"string\", \"null\"],\n                    \"enum\": [\"meta_analysis\", \"systematic_review\", \"guideline\", \"RCT\", \"observational\", \"label_claim\", \"expert_opinion\", None]\n                }\n            },\n            \"additionalProperties\": False\n        },\n        \"result_status\": {\n            \"type\": \"string\",\n            \"enum\": [\"found\", \"partial\", \"not_found\", \"ambiguous\", \"contradictory\"]\n        },\n        \"sources\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"title\"],\n                \"properties\": {\n                    \"title\": {\"type\": \"string\"},\n                    \"journal_or_publisher\": {\"type\": [\"string\", \"null\"]},\n                    \"year\": {\"type\": [\"integer\", \"null\"]},\n                    \"url\": {\"type\": [\"string\", \"null\"], \"format\": \"uri\"},\n                    \"doi\": {\"type\": [\"string\", \"null\"]},\n                    \"source_priority\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 4},\n                    \"needs_human_review\": {\"type\": \"boolean\"}\n                },\n                \"additionalProperties\": False\n            }\n        },\n        \"citations\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"type\", \"source_priority\"],\n                \"properties\": {\n                    \"type\": {\"type\": \"string\", \"enum\": [\"origin\", \"active_compounds\", \"daily_dose\", \"other\"]},\n                    \"title\": {\"type\": [\"string\", \"null\"]},\n                    \"journal_or_publisher\": {\"type\": [\"string\", \"null\"]},\n                    \"year\": {\"type\": [\"integer\", \"null\"]},\n                    \"url\": {\"type\": [\"string\", \"null\"], \"format\": \"uri\"},\n                    \"doi\": {\"type\": [\"string\", \"null\"]},\n                    \"exact_quote\": {\"type\": [\"string\", \"null\"], \"maxLength\": 1000},\n                    \"page_or_section\": {\"type\": [\"string\", \"null\"]},\n                    \"source_priority\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 4},\n                    \"needs_human_review\": {\"type\": \"boolean\"}\n                },\n                \"additionalProperties\": False\n            }\n        },\n        \"search_trace\": {\n            \"type\": [\"array\", \"null\"],\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"engine\": {\"type\": [\"string\", \"null\"]},\n                    \"query\": {\"type\": [\"string\", \"null\"]},\n                    \"results_checked\": {\"type\": [\"integer\", \"null\"]},\n                    \"notes\": {\"type\": [\"string\", \"null\"]}\n                },\n                \"additionalProperties\": False\n            }\n        },\n        \"provenance\": {\n            \"type\": \"object\",\n            \"required\": [\"colab_cell\", \"model\", \"retrieved_at\"],\n            \"properties\": {\n                \"colab_cell\": {\"type\": \"string\"},\n                \"model\": {\"type\": \"string\"},\n                \"model_version\": {\"type\": [\"string\", \"null\"]},\n                \"parser_version\": {\"type\": [\"string\", \"null\"]},\n                \"run_id\": {\"type\": [\"string\", \"null\"]},\n                \"retrieved_at\": {\"type\": \"string\", \"format\": \"date-time\"}\n            },\n            \"additionalProperties\": False\n        }\n    },\n    \"additionalProperties\": False\n}\n\nprint(\"üìã JSON Schema loaded\")\n\n# === VALIDATION FUNCTION ===\ndef validate_json_output(data):\n    \"\"\"–í–∞–ª—ñ–¥—É—î JSON –∑–∞ —Å—Ö–µ–º–æ—é\"\"\"\n    try:\n        jsonschema.validate(instance=data, schema=EVIDENCE_SCHEMA)\n        return True, None\n    except jsonschema.ValidationError as e:\n        return False, str(e)\n\nprint(\"‚úÖ OpenAI API and JSON Schema validation ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cell_3_search_verify"
   },
   "outputs": [],
   "source": "#@title Cell 3 ‚Äî Search and Verification Functions\n#@markdown –û—Å–Ω–æ–≤–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –ø–æ—à—É–∫—É —Ç–∞ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö\n\nimport requests\nimport time\nfrom urllib.parse import urlparse\nimport hashlib\n\n# === SYSTEM PROMPT ===\n# –ó 04_system_prompt_json_schema.md\nSYSTEM_PROMPT = \"\"\"\n–†–æ–ª—å: –ù–∞—É–∫–æ–≤–∏–π –µ–∫—Å–ø–µ—Ä—Ç –∑ —Ñ–∞—Ä–º–∞–∫–æ–≥–Ω–æ–∑—ñ—ó —Ç–∞ –Ω—É—Ç—Ä–∏—Ü—ñ–æ–ª–æ–≥—ñ—ó, —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç –∑ –ë–ê–î —ñ –º–µ–¥–∏—á–Ω–∏—Ö —Å–ø–æ–ª—É–∫.\n–ú–æ–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞. –¢–µ—Ö–Ω—ñ—á–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏ –ø–µ—Ä–µ–∫–ª–∞–¥–∞—Ç–∏ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ.\n\n–û–ë–û–í'–Ø–ó–ö–û–í–û –î–û–¢–†–ò–ú–£–ô–°–Ø:\n1. –ù—É–ª—å –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ–π. –Ø–∫—â–æ –¥–∞–Ω–∏—Ö –Ω–µ–º–∞—î, –ø–æ–≤–µ—Ä—Ç–∞–π result_status=\"not_found\" —ñ –ù–ï –≤–∏–≥–∞–¥—É–π URL/—Ü–∏—Ç–∞—Ç–∏.\n2. –¢–æ—á–Ω—ñ —Ü–∏—Ç–∞—Ç–∏. –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è –¥–æ–¥–∞–≤–∞–π citations[] –∑ —Ç–æ—á–Ω–æ—é —Ü–∏—Ç–∞—Ç–æ—é –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é.\n3. –ü—Ä—ñ–æ—Ä–∏—Ç–µ–∑–∞—Ü—ñ—è –¥–∂–µ—Ä–µ–ª: Level 1-4 (–Ω–∞–π–≤–∏—â–∞ –¥–æ –Ω–∞–π–Ω–∏–∂—á–æ—ó –¥–æ–≤—ñ—Ä–∏).\n4. –í–∞–ª—ñ–¥–∞—Ü—ñ—è –æ–¥–∏–Ω–∏—Ü—å. –î–æ–∑–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–æ –º–≥, –º–∫–≥, –≥, –ú–û, –º–≥/–∫–≥, –º–≥/–¥–µ–Ω—å, –º–∫–≥/–¥–µ–Ω—å.\n5. –§–æ—Ä–º–∞—Ç –≤–∏—Ö–æ–¥—É: —Å—É—Ç–æ JSON —â–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –≤–∞–ª—ñ–¥–∞—Ü—ñ—é.\n\n–ü–û–õ–Ü–¢–ò–ö–ê –î–ñ–ï–†–ï–õ:\nLevel 1: –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –æ–≥–ª—è–¥–∏, EFSA, FDA, NIH/ODS, WHO\nLevel 2: RCT —É —Ä–µ—Ü–µ–Ω–∑–æ–≤–∞–Ω–∏—Ö –∂—É—Ä–Ω–∞–ª–∞—Ö\nLevel 3: –†–µ—Ü–µ–Ω–∑–æ–≤–∞–Ω—ñ –∂—É—Ä–Ω–∞–ª–∏ 2-3 –∫–≤–∞—Ä—Ç–∏–ª—ñ\nLevel 4: –í–∏—Ä–æ–±–Ω–∏–∫–∏, –µ—Ç–∏–∫–µ—Ç–∫–∏ (needs_human_review=true)\n\n–î–û–ó–í–û–õ–ï–ù–Ü –î–û–ú–ï–ù–ò: nih.gov, ncbi.nlm.nih.gov, efsa.europa.eu, examine.com, consumerlab.com, sciencedirect.com, nature.com\n\n–°–£–í–û–†–ê JSON –°–¢–†–£–ö–¢–£–†–ê:\n{\n  \"ingredient_name_uk\": \"string (–æ–±–æ–≤'—è–∑–∫–æ–≤–æ)\",\n  \"ingredient_name_lat\": \"string (–æ–±–æ–≤'—è–∑–∫–æ–≤–æ)\", \n  \"synonyms\": [\"string array\"],\n  \"source_material\": {\n    \"kingdom\": \"one of: –†–æ—Å–ª–∏–Ω–∏|–¢–≤–∞—Ä–∏–Ω–∏|–ì—Ä–∏–±–∏|–ü—Ä–æ—Ç–∏—Å—Ç–∏|–ë–∞–∫—Ç–µ—Ä—ñ—ó|–ê—Ä—Ö–µ—ó|–ú—ñ–Ω–µ—Ä–∞–ª–∏|–°–∏–Ω—Ç–µ—Ç–∏—á–Ω–µ\",\n    \"part_or_origin\": \"string or null\",\n    \"description\": \"string or null\"\n  },\n  \"active_compounds\": [{\n    \"name\": \"string (–æ–±–æ–≤'—è–∑–∫–æ–≤–æ)\",\n    \"iupac\": \"string or null\",\n    \"cas\": \"string or null\", \n    \"concentration\": {\n      \"value\": number,\n      \"min_value\": number,\n      \"max_value\": number,\n      \"unit\": \"one of: %|–º–≥/–≥|–º–∫–≥/–≥|–º–≥/–º–ª|–º–∫–≥/–º–ª|ppm\"\n    }\n  }],\n  \"daily_dose\": {\n    \"recommended\": {\n      \"value\": number,\n      \"min_value\": number, \n      \"max_value\": number,\n      \"unit\": \"one of: –º–≥/–¥–µ–Ω—å|–≥/–¥–µ–Ω—å|–º–∫–≥/–¥–µ–Ω—å|–ú–û/–¥–µ–Ω—å|–º–≥/–∫–≥/–¥–µ–Ω—å\"\n    },\n    \"upper_limit\": {\n      \"value\": number,\n      \"min_value\": number,\n      \"max_value\": number, \n      \"unit\": \"one of: –º–≥/–¥–µ–Ω—å|–≥/–¥–µ–Ω—å|–º–∫–≥/–¥–µ–Ω—å|–ú–û/–¥–µ–Ω—å|–º–≥/–∫–≥/–¥–µ–Ω—å\"\n    },\n    \"evidence_type\": \"one of: meta_analysis|systematic_review|guideline|RCT|observational|label_claim|expert_opinion|null\"\n  },\n  \"result_status\": \"one of: found|partial|not_found|ambiguous|contradictory\",\n  \"sources\": [{\n    \"title\": \"string (–û–ë–û–í'–Ø–ó–ö–û–í–û)\",\n    \"journal_or_publisher\": \"string or null\",\n    \"year\": integer,\n    \"url\": \"string or null\",\n    \"doi\": \"string or null\", \n    \"source_priority\": integer (1-4),\n    \"needs_human_review\": boolean\n  }],\n  \"citations\": [{\n    \"type\": \"one of: origin|active_compounds|daily_dose|other\",\n    \"title\": \"string or null\",\n    \"journal_or_publisher\": \"string or null\",\n    \"year\": integer,\n    \"url\": \"string or null\",\n    \"doi\": \"string or null\",\n    \"exact_quote\": \"string (–¥–æ 1000 —Å–∏–º–≤–æ–ª—ñ–≤)\",\n    \"page_or_section\": \"string or null\",\n    \"source_priority\": integer (1-4, –û–ë–û–í'–Ø–ó–ö–û–í–û)\",\n    \"needs_human_review\": boolean\n  }],\n  \"search_trace\": [{\n    \"engine\": \"string\", \n    \"query\": \"string\",\n    \"results_checked\": integer,\n    \"notes\": \"string\"\n  }]\n}\n\n–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û:\n- sources –ó–ê–í–ñ–î–ò –ø–æ–≤–∏–Ω–Ω—ñ –º–∞—Ç–∏ –ø–æ–ª–µ \"title\" (–Ω–µ \"level\" —á–∏ \"description\")\n- citations –ó–ê–í–ñ–î–ò –ø–æ–≤–∏–Ω–Ω—ñ –º–∞—Ç–∏ –ø–æ–ª—è \"type\" —Ç–∞ \"source_priority\"\n- source_priority —Ç—ñ–ª—å–∫–∏ —á–∏—Å–ª–∞ 1, 2, 3, –∞–±–æ 4\n- needs_human_review —Ç—ñ–ª—å–∫–∏ true –∞–±–æ false\n- evidence_type –¢–Ü–õ–¨–ö–ò: meta_analysis, systematic_review, guideline, RCT, observational, label_claim, expert_opinion, –∞–±–æ null\n- –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π \"not_found\" –¥–ª—è evidence_type - –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π null\n- –Ø–∫—â–æ –¥–∞–Ω–∏—Ö –Ω–µ–º–∞—î - –ø–æ—Ä–æ–∂–Ω—ñ –º–∞—Å–∏–≤–∏ [], –Ω–µ null –¥–ª—è –º–∞—Å–∏–≤—ñ–≤\n- –Ø–∫—â–æ –Ω–µ–º–∞—î daily_dose –¥–∞–Ω–∏—Ö - –≤—Å—Ç–∞–Ω–æ–≤–∏ daily_dose: null\n\n–ü–æ–≤–µ—Ä—Ç–∞–π –¢–Ü–õ–¨–ö–ò –≤–∞–ª—ñ–¥–Ω–∏–π JSON –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –∞–±–æ –ø–æ—è—Å–Ω–µ–Ω—å.\n\"\"\"\n\ndef create_user_prompt(ingredient_name, synonyms=None):\n    \"\"\"–°—Ç–≤–æ—Ä—é—î –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∞\"\"\"\n    prompt = f\"\"\"\n–ó–Ω–∞–π–¥–∏ –Ω–∞—É–∫–æ–≤—ñ –¥–∞–Ω—ñ –ø—Ä–æ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç: {ingredient_name}\n\"\"\"\n    \n    if synonyms:\n        prompt += f\"\\n–°–∏–Ω–æ–Ω—ñ–º–∏: {', '.join(synonyms[:5])}\"  # –æ–±–º–µ–∂–∏—Ç–∏ –¥–æ 5 —Å–∏–Ω–æ–Ω—ñ–º—ñ–≤\n    \n    prompt += f\"\"\"\n\n–®—É–∫–∞–π —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ:\n1. –ü–æ—Ö–æ–¥–∂–µ–Ω–Ω—è (source_material): —Ü–∞—Ä—Å—Ç–≤–æ, —á–∞—Å—Ç–∏–Ω–∞ —Ä–æ—Å–ª–∏–Ω–∏/—Ç–≤–∞—Ä–∏–Ω–∏\n2. –ê–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏ (active_compounds): –Ω–∞–∑–≤–∏, CAS/IUPAC —è–∫—â–æ —î, –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó\n3. –î–æ–±–æ–≤—ñ –¥–æ–∑–∏ (daily_dose): —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ —Ç–∞ –≤–µ—Ä—Ö–Ω—ñ –º–µ–∂—ñ –∑ –æ–¥–∏–Ω–∏—Ü—è–º–∏\n\n–û–ë–û–í'–Ø–ó–ö–û–í–û:\n- –ó–∞–ø–æ–≤–Ω–∏ ingredient_name_uk: \"{ingredient_name}\" (—É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é)\n- –ó–∞–ø–æ–≤–Ω–∏ ingredient_name_lat: \"–Ω–∞—É–∫–æ–≤–∞ –ª–∞—Ç–∏–Ω—Å—å–∫–∞ –Ω–∞–∑–≤–∞\"\n- –î–ª—è sources –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò: title, journal_or_publisher, year, url, doi, source_priority (1-4), needs_human_review\n- –î–ª—è citations –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò: type, title, journal_or_publisher, year, url, doi, exact_quote, page_or_section, source_priority (1-4), needs_human_review\n- evidence_type –¢–Ü–õ–¨–ö–ò: meta_analysis, systematic_review, guideline, RCT, observational, label_claim, expert_opinion, –∞–±–æ null\n- –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –ø–æ–ª—è level, description, \"not_found\" —á–∏ —ñ–Ω—à—ñ –Ω–µ–æ—á—ñ–∫—É–≤–∞–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è\n\n–ü–æ–≤–µ—Ä—Ç–∞–π –ª–∏—à–µ –≤–∞–ª—ñ–¥–Ω–∏–π JSON –∑–≥—ñ–¥–Ω–æ –∑—ñ —Å—Ö–µ–º–æ—é –≤–∏—â–µ.\n\"\"\"\n    \n    return prompt\n\ndef fetch_evidence_via_openai(ingredient_name, synonyms=None):\n    \"\"\"–û—Ç—Ä–∏–º—É—î –¥–∞–Ω—ñ —á–µ—Ä–µ–∑ OpenAI API\"\"\"\n    try:\n        user_prompt = create_user_prompt(ingredient_name, synonyms)\n        \n        response = client.chat.completions.create(\n            model=OPENAI_MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=TEMPERATURE,\n            max_tokens=MAX_TOKENS,\n            response_format={\"type\": \"json_object\"}  # –ü—Ä–∏–º—É—Å–∏—Ç–∏ JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—å\n        )\n        \n        content = response.choices[0].message.content\n        \n        # –ü–∞—Ä—Å–∏–Ω–≥ JSON\n        try:\n            data = json.loads(content)\n        except json.JSONDecodeError as e:\n            print(f\"‚ùå JSON parsing error for {ingredient_name}: {e}\")\n            return None\n        \n        # –î–æ–¥–∞—Ç–∏ provenance\n        data[\"provenance\"] = {\n            \"colab_cell\": \"Cell 3 ‚Äî Evidence Fetch\",\n            \"model\": OPENAI_MODEL,\n            \"model_version\": response.model,\n            \"run_id\": RUN_ID,\n            \"retrieved_at\": datetime.datetime.now().isoformat()\n        }\n        \n        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è\n        is_valid, error = validate_json_output(data)\n        if not is_valid:\n            print(f\"‚ùå Schema validation failed for {ingredient_name}: {error}\")\n            print(f\"üìã Returned data structure: {list(data.keys()) if isinstance(data, dict) else 'not dict'}\")\n            if isinstance(data, dict) and 'sources' in data:\n                print(f\"üìã Sources structure: {[list(s.keys()) if isinstance(s, dict) else s for s in data['sources'][:2]]}\")\n            return None\n        \n        return data\n        \n    except Exception as e:\n        print(f\"‚ùå OpenAI API error for {ingredient_name}: {e}\")\n        return None\n\ndef verify_citations(data):\n    \"\"\"–ü–µ—Ä–µ–≤—ñ—Ä—è—î –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å URL —É —Ü–∏—Ç–∞—Ç–∞—Ö\"\"\"\n    if not data or \"citations\" not in data:\n        return data\n    \n    verified_citations = []\n    \n    for citation in data[\"citations\"]:\n        citation[\"verified\"] = False\n        \n        url = citation.get(\"url\")\n        if url:\n            try:\n                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ–º–µ–Ω—É\n                domain = urlparse(url).netloc.lower()\n                domain_allowed = any(allowed in domain for allowed in ALLOWED_DOMAINS)\n                \n                if domain_allowed:\n                    # HTTP –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ (—Ç—ñ–ª—å–∫–∏ HEAD –∑–∞–ø–∏—Ç)\n                    response = requests.head(url, timeout=10, allow_redirects=True)\n                    if response.status_code == 200:\n                        citation[\"verified\"] = True\n                    else:\n                        print(f\"‚ö†Ô∏è  URL not accessible: {url} (status: {response.status_code})\")\n                else:\n                    print(f\"‚ö†Ô∏è  Domain not allowed: {domain}\")\n                    citation[\"needs_human_review\"] = True\n                    \n            except Exception as e:\n                print(f\"‚ö†Ô∏è  URL verification failed: {url} - {e}\")\n        \n        verified_citations.append(citation)\n    \n    data[\"citations\"] = verified_citations\n    return data\n\ndef fetch_and_verify(ingredient_name, synonyms=None):\n    \"\"\"–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è: –æ—Ç—Ä–∏–º—É—î —Ç–∞ –≤–µ—Ä–∏—Ñ—ñ–∫—É—î –¥–∞–Ω—ñ\"\"\"\n    print(f\"üîç Processing: {ingredient_name}\")\n    \n    # –û—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞–Ω—ñ —á–µ—Ä–µ–∑ OpenAI\n    data = fetch_evidence_via_openai(ingredient_name, synonyms)\n    \n    if not data:\n        return None\n    \n    # –í–µ—Ä–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ —Ü–∏—Ç–∞—Ç–∏\n    data = verify_citations(data)\n    \n    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó\n    total_citations = len(data.get(\"citations\", []))\n    verified_citations = sum(1 for c in data.get(\"citations\", []) if c.get(\"verified\", False))\n    \n    print(f\"‚úÖ Completed {ingredient_name}: {verified_citations}/{total_citations} citations verified\")\n    \n    return data\n\nprint(\"üîç Search and verification functions ready\")\nprint(\"üìã System prompt loaded with detailed JSON schema requirements\")\nprint(\"‚úÖ Cell 3 completed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cell_4_batch_processing"
   },
   "outputs": [],
   "source": [
    "#@title Cell 4 ‚Äî Batch Processing\n",
    "#@markdown –ú–∞—Å–æ–≤–∞ –æ–±—Ä–æ–±–∫–∞ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤ –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–æ–º–∏–ª–æ–∫\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# === BATCH SETTINGS ===\n",
    "BATCH_SIZE = 10  # –æ–±—Ä–æ–±–ª—è—Ç–∏ –ø–æ 10 —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤ –∑–∞ —Ä–∞–∑\n",
    "DELAY_BETWEEN_REQUESTS = 1  # –∑–∞—Ç—Ä–∏–º–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏\n",
    "MAX_RETRIES = 3  # –º–∞–∫—Å–∏–º—É–º —Å–ø—Ä–æ–± –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ\n",
    "CHECKPOINT_FREQUENCY = 50  # –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –ø—Ä–æ–º—ñ–∂–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–æ–∂–Ω—ñ 50 –µ–ª–µ–º–µ–Ω—Ç—ñ–≤\n",
    "\n",
    "def process_ingredient_batch(ingredients_batch, start_index=0):\n",
    "    \"\"\"–û–±—Ä–æ–±–ª—è—î batch —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤\"\"\"\n",
    "    results = []\n",
    "    errors = []\n",
    "    \n",
    "    for i, ingredient in enumerate(tqdm(ingredients_batch, desc=\"Processing batch\")):\n",
    "        current_index = start_index + i\n",
    "        \n",
    "        # –ü–æ—à—É–∫ —Å–∏–Ω–æ–Ω—ñ–º—ñ–≤ –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∞\n",
    "        ingredient_synonyms = []\n",
    "        if current_index < len(synonyms_list):\n",
    "            synonym_entry = synonyms_list[current_index]\n",
    "            if synonym_entry:\n",
    "                ingredient_synonyms = [s.strip() for s in synonym_entry.split(',') if s.strip()]\n",
    "        \n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < MAX_RETRIES and not success:\n",
    "            try:\n",
    "                result = fetch_and_verify(ingredient, ingredient_synonyms)\n",
    "                \n",
    "                if result:\n",
    "                    results.append({\n",
    "                        \"index\": current_index,\n",
    "                        \"ingredient\": ingredient,\n",
    "                        \"data\": result,\n",
    "                        \"timestamp\": datetime.datetime.now().isoformat()\n",
    "                    })\n",
    "                    success = True\n",
    "                else:\n",
    "                    retry_count += 1\n",
    "                    if retry_count < MAX_RETRIES:\n",
    "                        print(f\"‚ö†Ô∏è  Retry {retry_count}/{MAX_RETRIES} for {ingredient}\")\n",
    "                        time.sleep(DELAY_BETWEEN_REQUESTS * 2)  # –ø–æ–¥–≤—ñ–π–Ω–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –ø—Ä–∏ —Ä–µ—Ç—Ä–∞—ó\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                error_info = {\n",
    "                    \"index\": current_index,\n",
    "                    \"ingredient\": ingredient,\n",
    "                    \"error\": str(e),\n",
    "                    \"retry\": retry_count,\n",
    "                    \"timestamp\": datetime.datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                if retry_count >= MAX_RETRIES:\n",
    "                    errors.append(error_info)\n",
    "                    print(f\"‚ùå Failed after {MAX_RETRIES} retries: {ingredient} - {e}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  Error on retry {retry_count}: {ingredient} - {e}\")\n",
    "                    time.sleep(DELAY_BETWEEN_REQUESTS * 2)\n",
    "        \n",
    "        # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏\n",
    "        if i < len(ingredients_batch) - 1:  # –Ω–µ —á–µ–∫–∞—Ç–∏ –ø—ñ—Å–ª—è –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç—É\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "    \n",
    "    return results, errors\n",
    "\n",
    "def save_checkpoint(results, errors, checkpoint_index):\n",
    "    \"\"\"–ó–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–º—ñ–∂–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏\"\"\"\n",
    "    checkpoint_file = f\"{RESULTS_DIR}/checkpoint_{checkpoint_index}.pkl\"\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        \"results\": results,\n",
    "        \"errors\": errors,\n",
    "        \"checkpoint_index\": checkpoint_index,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"run_id\": RUN_ID\n",
    "    }\n",
    "    \n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(checkpoint_data, f)\n",
    "    \n",
    "    print(f\"üíæ Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_index):\n",
    "    \"\"\"–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –ø—Ä–æ–º—ñ–∂–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏\"\"\"\n",
    "    checkpoint_file = f\"{RESULTS_DIR}/checkpoint_{checkpoint_index}.pkl\"\n",
    "    \n",
    "    try:\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def run_full_batch_processing(start_from_checkpoint=None):\n",
    "    \"\"\"–ó–∞–ø—É—Å–∫–∞—î –ø–æ–≤–Ω—É –æ–±—Ä–æ–±–∫—É –≤—Å—ñ—Ö —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤\"\"\"\n",
    "    print(f\"üöÄ Starting batch processing of {len(query_terms)} ingredients\")\n",
    "    print(f\"‚öôÔ∏è  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"‚è±Ô∏è  Delay between requests: {DELAY_BETWEEN_REQUESTS}s\")\n",
    "    print(f\"üîÑ Max retries: {MAX_RETRIES}\")\n",
    "    \n",
    "    all_results = []\n",
    "    all_errors = []\n",
    "    start_index = 0\n",
    "    \n",
    "    # –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ checkpoint\n",
    "    if start_from_checkpoint:\n",
    "        checkpoint_data = load_checkpoint(start_from_checkpoint)\n",
    "        if checkpoint_data:\n",
    "            all_results = checkpoint_data[\"results\"]\n",
    "            all_errors = checkpoint_data[\"errors\"]\n",
    "            start_index = checkpoint_data[\"checkpoint_index\"]\n",
    "            print(f\"üîÑ Resuming from checkpoint {start_from_checkpoint} (index {start_index})\")\n",
    "    \n",
    "    # –û–±—Ä–æ–±–∫–∞ batches\n",
    "    total_batches = (len(query_terms) - start_index + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        batch_start = start_index + batch_num * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(query_terms))\n",
    "        \n",
    "        print(f\"\\nüì¶ Processing batch {batch_num + 1}/{total_batches} (items {batch_start}-{batch_end-1})\")\n",
    "        \n",
    "        batch_ingredients = query_terms[batch_start:batch_end]\n",
    "        batch_results, batch_errors = process_ingredient_batch(batch_ingredients, batch_start)\n",
    "        \n",
    "        all_results.extend(batch_results)\n",
    "        all_errors.extend(batch_errors)\n",
    "        \n",
    "        # Checkpoint –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è\n",
    "        if (batch_num + 1) % (CHECKPOINT_FREQUENCY // BATCH_SIZE) == 0 or batch_end >= len(query_terms):\n",
    "            save_checkpoint(all_results, all_errors, batch_end)\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—É\n",
    "        processed = len(all_results)\n",
    "        failed = len(all_errors)\n",
    "        total_processed = processed + failed\n",
    "        \n",
    "        print(f\"üìä Progress: {total_processed}/{len(query_terms)} processed, {processed} successful, {failed} failed\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Batch processing completed!\")\n",
    "    print(f\"üìä Final stats: {len(all_results)} successful, {len(all_errors)} failed\")\n",
    "    \n",
    "    return all_results, all_errors\n",
    "\n",
    "# === TEST RUN ===\n",
    "def run_test_batch(num_items=5):\n",
    "    \"\"\"–¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø—É—Å–∫ –Ω–∞ –Ω–µ–≤–µ–ª–∏–∫—ñ–π –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤\"\"\"\n",
    "    print(f\"üß™ Running test batch with {num_items} ingredients\")\n",
    "    \n",
    "    test_ingredients = query_terms[:num_items]\n",
    "    test_results, test_errors = process_ingredient_batch(test_ingredients)\n",
    "    \n",
    "    print(f\"\\nüß™ Test completed: {len(test_results)} successful, {len(test_errors)} failed\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É\n",
    "    if test_results:\n",
    "        print(\"\\nüìã Sample result:\")\n",
    "        sample = test_results[0]\n",
    "        print(f\"  Ingredient: {sample['ingredient']}\")\n",
    "        print(f\"  Status: {sample['data'].get('result_status', 'unknown')}\")\n",
    "        print(f\"  Citations: {len(sample['data'].get('citations', []))}\")\n",
    "    \n",
    "    return test_results, test_errors\n",
    "\n",
    "print(\"‚ö° Batch processing functions ready\")\n",
    "print(\"üß™ Use run_test_batch(5) for testing\")\n",
    "print(\"üöÄ Use run_full_batch_processing() for full processing\")\n",
    "print(\"‚úÖ Cell 4 completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cell_5_results_export"
   },
   "outputs": [],
   "source": [
    "#@title Cell 5 ‚Äî Results Export\n",
    "#@markdown –ï–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É Google Sheets, JSONL —Ç–∞ CSV\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import gspread\n",
    "from google.colab import files\n",
    "\n",
    "def export_to_jsonl(results, filename=None):\n",
    "    \"\"\"–ï–∫—Å–ø–æ—Ä—Ç —É JSONL —Ñ–æ—Ä–º–∞—Ç\"\"\"\n",
    "    if not filename:\n",
    "        filename = f\"{RESULTS_DIR}/dlsd_results_{RUN_ID}.jsonl\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for result in results:\n",
    "            # –ó–∞–ø–∏—Å–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –¥–∞–Ω—ñ –±–µ–∑ –º–µ—Ç–∞—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó\n",
    "            json.dump(result['data'], f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"üìÑ JSONL exported: {filename} ({len(results)} records)\")\n",
    "    return filename\n",
    "\n",
    "def export_to_csv(results, filename=None):\n",
    "    \"\"\"–ï–∫—Å–ø–æ—Ä—Ç —É CSV —Ñ–æ—Ä–º–∞—Ç (—Å–ø—Ä–æ—â–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—è)\"\"\"\n",
    "    if not filename:\n",
    "        filename = f\"{RESULTS_DIR}/dlsd_results_{RUN_ID}.csv\"\n",
    "    \n",
    "    # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –¥–ª—è CSV\n",
    "    csv_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        data = result['data']\n",
    "        \n",
    "        # –ë–∞–∑–æ–≤—ñ –ø–æ–ª—è\n",
    "        row = {\n",
    "            'index': result.get('index', ''),\n",
    "            'ingredient_name_uk': data.get('ingredient_name_uk', ''),\n",
    "            'ingredient_name_lat': data.get('ingredient_name_lat', ''),\n",
    "            'result_status': data.get('result_status', ''),\n",
    "            'kingdom': data.get('source_material', {}).get('kingdom', '') if data.get('source_material') else '',\n",
    "            'part_or_origin': data.get('source_material', {}).get('part_or_origin', '') if data.get('source_material') else '',\n",
    "        }\n",
    "        \n",
    "        # –ê–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏ (–ø–µ—Ä—à—ñ 3)\n",
    "        compounds = data.get('active_compounds', [])\n",
    "        for i in range(3):\n",
    "            if i < len(compounds):\n",
    "                row[f'compound_{i+1}_name'] = compounds[i].get('name', '')\n",
    "                row[f'compound_{i+1}_cas'] = compounds[i].get('cas', '')\n",
    "            else:\n",
    "                row[f'compound_{i+1}_name'] = ''\n",
    "                row[f'compound_{i+1}_cas'] = ''\n",
    "        \n",
    "        # –î–æ–±–æ–≤—ñ –¥–æ–∑–∏\n",
    "        daily_dose = data.get('daily_dose', {})\n",
    "        if daily_dose and daily_dose.get('recommended'):\n",
    "            rec = daily_dose['recommended']\n",
    "            row['recommended_dose_min'] = rec.get('min_value', '')\n",
    "            row['recommended_dose_max'] = rec.get('max_value', '')\n",
    "            row['recommended_dose_value'] = rec.get('value', '')\n",
    "            row['recommended_dose_unit'] = rec.get('unit', '')\n",
    "        else:\n",
    "            row['recommended_dose_min'] = ''\n",
    "            row['recommended_dose_max'] = ''\n",
    "            row['recommended_dose_value'] = ''\n",
    "            row['recommended_dose_unit'] = ''\n",
    "        \n",
    "        # –ú–µ—Ç–∞–¥–∞–Ω—ñ\n",
    "        row['total_sources'] = len(data.get('sources', []))\n",
    "        row['total_citations'] = len(data.get('citations', []))\n",
    "        row['verified_citations'] = len([c for c in data.get('citations', []) if c.get('verified', False)])\n",
    "        row['needs_human_review'] = any(c.get('needs_human_review', False) for c in data.get('citations', []))\n",
    "        row['retrieved_at'] = data.get('provenance', {}).get('retrieved_at', '')\n",
    "        \n",
    "        csv_data.append(row)\n",
    "    \n",
    "    # –°—Ç–≤–æ—Ä–∏—Ç–∏ DataFrame —Ç–∞ –∑–±–µ—Ä–µ–≥—Ç–∏\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"üìä CSV exported: {filename} ({len(csv_data)} records, {len(df.columns)} columns)\")\n",
    "    return filename\n",
    "\n",
    "def create_results_sheet_if_not_exists():\n",
    "    \"\"\"–°—Ç–≤–æ—Ä—é—î –∞—Ä–∫—É—à Results_Main —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î\"\"\"\n",
    "    try:\n",
    "        workbook = gc.open_by_key(SHEET_ID_ING)\n",
    "        \n",
    "        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —ñ—Å–Ω—É—î –∞—Ä–∫—É—à\n",
    "        try:\n",
    "            results_sheet = workbook.worksheet(\"Results_Main\")\n",
    "            print(\"üìã Results_Main sheet already exists\")\n",
    "            return results_sheet\n",
    "        except gspread.WorksheetNotFound:\n",
    "            # –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π –∞—Ä–∫—É—à\n",
    "            results_sheet = workbook.add_worksheet(title=\"Results_Main\", rows=10000, cols=50)\n",
    "            \n",
    "            # –î–æ–¥–∞—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏\n",
    "            headers = [\n",
    "                'Index', 'Ingredient_UK', 'Ingredient_LAT', 'Status', 'Kingdom', 'Part_Origin',\n",
    "                'Compound_1', 'CAS_1', 'Compound_2', 'CAS_2', 'Compound_3', 'CAS_3',\n",
    "                'Dose_Min', 'Dose_Max', 'Dose_Value', 'Dose_Unit',\n",
    "                'Total_Sources', 'Total_Citations', 'Verified_Citations', 'Needs_Review',\n",
    "                'Retrieved_At', 'Run_ID'\n",
    "            ]\n",
    "            \n",
    "            results_sheet.update('A1', [headers])\n",
    "            print(\"üìã Results_Main sheet created with headers\")\n",
    "            return results_sheet\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating Results_Main sheet: {e}\")\n",
    "        return None\n",
    "\n",
    "def export_to_google_sheets(results):\n",
    "    \"\"\"–ï–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É Google Sheets\"\"\"\n",
    "    results_sheet = create_results_sheet_if_not_exists()\n",
    "    \n",
    "    if not results_sheet:\n",
    "        print(\"‚ùå Cannot export to Google Sheets - sheet creation failed\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –¥–ª—è Google Sheets\n",
    "        sheet_data = []\n",
    "        \n",
    "        for result in results:\n",
    "            data = result['data']\n",
    "            \n",
    "            # –ê–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏\n",
    "            compounds = data.get('active_compounds', [])\n",
    "            compound_names = [c.get('name', '') for c in compounds[:3]]\n",
    "            compound_cas = [c.get('cas', '') for c in compounds[:3]]\n",
    "            \n",
    "            # –î–æ–¥–∞—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —è–∫—â–æ –º–µ–Ω—à–µ 3 —Å–ø–æ–ª—É–∫\n",
    "            while len(compound_names) < 3:\n",
    "                compound_names.append('')\n",
    "                compound_cas.append('')\n",
    "            \n",
    "            # –î–æ–±–æ–≤—ñ –¥–æ–∑–∏\n",
    "            daily_dose = data.get('daily_dose', {})\n",
    "            dose_info = ['', '', '', '']\n",
    "            if daily_dose and daily_dose.get('recommended'):\n",
    "                rec = daily_dose['recommended']\n",
    "                dose_info = [\n",
    "                    rec.get('min_value', ''),\n",
    "                    rec.get('max_value', ''),\n",
    "                    rec.get('value', ''),\n",
    "                    rec.get('unit', '')\n",
    "                ]\n",
    "            \n",
    "            row = [\n",
    "                result.get('index', ''),\n",
    "                data.get('ingredient_name_uk', ''),\n",
    "                data.get('ingredient_name_lat', ''),\n",
    "                data.get('result_status', ''),\n",
    "                data.get('source_material', {}).get('kingdom', '') if data.get('source_material') else '',\n",
    "                data.get('source_material', {}).get('part_or_origin', '') if data.get('source_material') else '',\n",
    "                *compound_names,\n",
    "                *compound_cas,\n",
    "                *dose_info,\n",
    "                len(data.get('sources', [])),\n",
    "                len(data.get('citations', [])),\n",
    "                len([c for c in data.get('citations', []) if c.get('verified', False)]),\n",
    "                any(c.get('needs_human_review', False) for c in data.get('citations', [])),\n",
    "                data.get('provenance', {}).get('retrieved_at', ''),\n",
    "                RUN_ID\n",
    "            ]\n",
    "            \n",
    "            sheet_data.append(row)\n",
    "        \n",
    "        # –ó–Ω–∞–π—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ä—è–¥–æ–∫ –∑ –¥–∞–Ω–∏–º–∏\n",
    "        existing_data = results_sheet.get_all_values()\n",
    "        last_row = len(existing_data)\n",
    "        \n",
    "        # –î–æ–¥–∞—Ç–∏ –Ω–æ–≤—ñ –¥–∞–Ω—ñ\n",
    "        if sheet_data:\n",
    "            start_cell = f\"A{last_row + 1}\"\n",
    "            results_sheet.update(start_cell, sheet_data)\n",
    "            \n",
    "            print(f\"üìä Google Sheets updated: {len(sheet_data)} records added starting from row {last_row + 1}\")\n",
    "            print(f\"üîó Sheet URL: https://docs.google.com/spreadsheets/d/{SHEET_ID_ING}/edit#gid={results_sheet.id}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting to Google Sheets: {e}\")\n",
    "        return False\n",
    "\n",
    "def export_all_formats(results, download_files=True):\n",
    "    \"\"\"–ï–∫—Å–ø–æ—Ä—Ç —É –≤—Å—ñ —Ñ–æ—Ä–º–∞—Ç–∏\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è  No results to export\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üì§ Starting export of {len(results)} results...\")\n",
    "    \n",
    "    # JSONL\n",
    "    jsonl_file = export_to_jsonl(results)\n",
    "    \n",
    "    # CSV\n",
    "    csv_file = export_to_csv(results)\n",
    "    \n",
    "    # Google Sheets\n",
    "    sheets_success = export_to_google_sheets(results)\n",
    "    \n",
    "    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "    if download_files:\n",
    "        try:\n",
    "            files.download(jsonl_file)\n",
    "            files.download(csv_file)\n",
    "            print(\"üíæ Files downloaded to local machine\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Download failed: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Export completed:\")\n",
    "    print(f\"  üìÑ JSONL: {jsonl_file}\")\n",
    "    print(f\"  üìä CSV: {csv_file}\")\n",
    "    print(f\"  üìã Google Sheets: {'‚úÖ Success' if sheets_success else '‚ùå Failed'}\")\n",
    "\n",
    "print(\"üì§ Export functions ready\")\n",
    "print(\"üíæ Use export_all_formats(results) to export in all formats\")\n",
    "print(\"‚úÖ Cell 5 completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cell_6_quality_assurance"
   },
   "outputs": [],
   "source": [
    "#@title Cell 6 ‚Äî Quality Assurance\n",
    "#@markdown –ê–Ω–∞–ª—ñ–∑ —è–∫–æ—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—ñ–≤\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def analyze_results_quality(results):\n",
    "    \"\"\"–ê–Ω–∞–ª—ñ–∑—É—î —è–∫—ñ—Å—Ç—å –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è  No results to analyze\")\n",
    "        return {}\n",
    "    \n",
    "    analysis = {\n",
    "        'total_processed': len(results),\n",
    "        'status_distribution': Counter(),\n",
    "        'source_priority_distribution': Counter(),\n",
    "        'verification_stats': {\n",
    "            'total_citations': 0,\n",
    "            'verified_citations': 0,\n",
    "            'needs_human_review': 0\n",
    "        },\n",
    "        'kingdom_distribution': Counter(),\n",
    "        'compounds_stats': {\n",
    "            'ingredients_with_compounds': 0,\n",
    "            'total_compounds': 0,\n",
    "            'avg_compounds_per_ingredient': 0\n",
    "        },\n",
    "        'dose_stats': {\n",
    "            'ingredients_with_doses': 0,\n",
    "            'evidence_types': Counter()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for result in results:\n",
    "        data = result['data']\n",
    "        \n",
    "        # –°—Ç–∞—Ç—É—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
    "        status = data.get('result_status', 'unknown')\n",
    "        analysis['status_distribution'][status] += 1\n",
    "        \n",
    "        # –ê–Ω–∞–ª—ñ–∑ –¥–∂–µ—Ä–µ–ª\n",
    "        for source in data.get('sources', []):\n",
    "            priority = source.get('source_priority', 0)\n",
    "            analysis['source_priority_distribution'][f'Level {priority}'] += 1\n",
    "        \n",
    "        # –ê–Ω–∞–ª—ñ–∑ —Ü–∏—Ç–∞—Ç\n",
    "        citations = data.get('citations', [])\n",
    "        analysis['verification_stats']['total_citations'] += len(citations)\n",
    "        \n",
    "        for citation in citations:\n",
    "            if citation.get('verified', False):\n",
    "                analysis['verification_stats']['verified_citations'] += 1\n",
    "            if citation.get('needs_human_review', False):\n",
    "                analysis['verification_stats']['needs_human_review'] += 1\n",
    "        \n",
    "        # –ê–Ω–∞–ª—ñ–∑ source_material\n",
    "        source_material = data.get('source_material', {})\n",
    "        if source_material:\n",
    "            kingdom = source_material.get('kingdom', 'Unknown')\n",
    "            analysis['kingdom_distribution'][kingdom] += 1\n",
    "        \n",
    "        # –ê–Ω–∞–ª—ñ–∑ –∞–∫—Ç–∏–≤–Ω–∏—Ö —Å–ø–æ–ª—É–∫\n",
    "        compounds = data.get('active_compounds', [])\n",
    "        if compounds:\n",
    "            analysis['compounds_stats']['ingredients_with_compounds'] += 1\n",
    "            analysis['compounds_stats']['total_compounds'] += len(compounds)\n",
    "        \n",
    "        # –ê–Ω–∞–ª—ñ–∑ –¥–æ–∑\n",
    "        daily_dose = data.get('daily_dose', {})\n",
    "        if daily_dose:\n",
    "            analysis['dose_stats']['ingredients_with_doses'] += 1\n",
    "            evidence_type = daily_dose.get('evidence_type', 'unknown')\n",
    "            analysis['dose_stats']['evidence_types'][evidence_type] += 1\n",
    "    \n",
    "    # –û–±—á–∏—Å–ª–∏—Ç–∏ —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è\n",
    "    if analysis['compounds_stats']['ingredients_with_compounds'] > 0:\n",
    "        analysis['compounds_stats']['avg_compounds_per_ingredient'] = (\n",
    "            analysis['compounds_stats']['total_compounds'] / \n",
    "            analysis['compounds_stats']['ingredients_with_compounds']\n",
    "        )\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def print_quality_report(analysis):\n",
    "    \"\"\"–í–∏–≤–æ–¥–∏—Ç—å –∑–≤—ñ—Ç –ø—Ä–æ —è–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä QUALITY ANALYSIS REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    print(f\"\\nüìà GENERAL STATISTICS\")\n",
    "    print(f\"  Total processed: {analysis['total_processed']}\")\n",
    "    \n",
    "    # –†–æ–∑–ø–æ–¥—ñ–ª —Å—Ç–∞—Ç—É—Å—ñ–≤\n",
    "    print(f\"\\nüìã RESULT STATUS DISTRIBUTION\")\n",
    "    for status, count in analysis['status_distribution'].most_common():\n",
    "        percentage = (count / analysis['total_processed']) * 100\n",
    "        print(f\"  {status}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–∏—Ç–∞—Ç\n",
    "    vs = analysis['verification_stats']\n",
    "    print(f\"\\nüîç CITATION VERIFICATION\")\n",
    "    print(f\"  Total citations: {vs['total_citations']}\")\n",
    "    print(f\"  Verified citations: {vs['verified_citations']}\")\n",
    "    print(f\"  Need human review: {vs['needs_human_review']}\")\n",
    "    \n",
    "    if vs['total_citations'] > 0:\n",
    "        verification_rate = (vs['verified_citations'] / vs['total_citations']) * 100\n",
    "        review_rate = (vs['needs_human_review'] / vs['total_citations']) * 100\n",
    "        print(f\"  Verification rate: {verification_rate:.1f}%\")\n",
    "        print(f\"  Human review rate: {review_rate:.1f}%\")\n",
    "    \n",
    "    # –†–æ–∑–ø–æ–¥—ñ–ª –¥–∂–µ—Ä–µ–ª –∑–∞ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º\n",
    "    print(f\"\\nüèÜ SOURCE PRIORITY DISTRIBUTION\")\n",
    "    for priority, count in sorted(analysis['source_priority_distribution'].items()):\n",
    "        print(f\"  {priority}: {count}\")\n",
    "    \n",
    "    # –†–æ–∑–ø–æ–¥—ñ–ª –∑–∞ —Ü–∞—Ä—Å—Ç–≤–∞–º–∏\n",
    "    print(f\"\\nüå± KINGDOM DISTRIBUTION\")\n",
    "    for kingdom, count in analysis['kingdom_distribution'].most_common():\n",
    "        percentage = (count / analysis['total_processed']) * 100\n",
    "        print(f\"  {kingdom}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ø–æ–ª—É–∫\n",
    "    cs = analysis['compounds_stats']\n",
    "    print(f\"\\nüß™ ACTIVE COMPOUNDS STATISTICS\")\n",
    "    print(f\"  Ingredients with compounds: {cs['ingredients_with_compounds']}\")\n",
    "    print(f\"  Total compounds found: {cs['total_compounds']}\")\n",
    "    print(f\"  Average compounds per ingredient: {cs['avg_compounds_per_ingredient']:.1f}\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∑\n",
    "    ds = analysis['dose_stats']\n",
    "    print(f\"\\nüíä DOSAGE STATISTICS\")\n",
    "    print(f\"  Ingredients with dose info: {ds['ingredients_with_doses']}\")\n",
    "    print(f\"  Evidence types:\")\n",
    "    for evidence_type, count in ds['evidence_types'].most_common():\n",
    "        print(f\"    {evidence_type}: {count}\")\n",
    "\n",
    "def create_quality_visualizations(analysis, save_plots=True):\n",
    "    \"\"\"–°—Ç–≤–æ—Ä—é—î –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —è–∫–æ—Å—Ç—ñ\"\"\"\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('DLSD Evidence Collector - Quality Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. –†–æ–∑–ø–æ–¥—ñ–ª —Å—Ç–∞—Ç—É—Å—ñ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
    "    statuses = list(analysis['status_distribution'].keys())\n",
    "    status_counts = list(analysis['status_distribution'].values())\n",
    "    \n",
    "    axes[0, 0].pie(status_counts, labels=statuses, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Result Status Distribution')\n",
    "    \n",
    "    # 2. –†–æ–∑–ø–æ–¥—ñ–ª –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—ñ–≤ –¥–∂–µ—Ä–µ–ª\n",
    "    priorities = list(analysis['source_priority_distribution'].keys())\n",
    "    priority_counts = list(analysis['source_priority_distribution'].values())\n",
    "    \n",
    "    if priorities:\n",
    "        axes[0, 1].bar(priorities, priority_counts, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "        axes[0, 1].set_title('Source Priority Distribution')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "        plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 3. –†–æ–∑–ø–æ–¥—ñ–ª –∑–∞ —Ü–∞—Ä—Å—Ç–≤–∞–º–∏\n",
    "    kingdoms = list(analysis['kingdom_distribution'].keys())\n",
    "    kingdom_counts = list(analysis['kingdom_distribution'].values())\n",
    "    \n",
    "    if kingdoms:\n",
    "        axes[1, 0].barh(kingdoms, kingdom_counts)\n",
    "        axes[1, 0].set_title('Kingdom Distribution')\n",
    "        axes[1, 0].set_xlabel('Count')\n",
    "    \n",
    "    # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó\n",
    "    vs = analysis['verification_stats']\n",
    "    verification_data = {\n",
    "        'Verified': vs['verified_citations'],\n",
    "        'Not Verified': vs['total_citations'] - vs['verified_citations'],\n",
    "        'Need Review': vs['needs_human_review']\n",
    "    }\n",
    "    \n",
    "    verification_labels = list(verification_data.keys())\n",
    "    verification_values = list(verification_data.values())\n",
    "    \n",
    "    axes[1, 1].bar(verification_labels, verification_values, \n",
    "                   color=['#2ca02c', '#d62728', '#ff7f0e'])\n",
    "    axes[1, 1].set_title('Citation Verification Status')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_plots:\n",
    "        plot_filename = f\"{RESULTS_DIR}/quality_analysis_{RUN_ID}.png\"\n",
    "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Quality analysis plot saved: {plot_filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def generate_quality_report(results, create_visualizations=True):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä—É—î –ø–æ–≤–Ω–∏–π –∑–≤—ñ—Ç –ø—Ä–æ —è–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\"\"\"\n",
    "    print(\"üîç Analyzing results quality...\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª—ñ–∑\n",
    "    analysis = analyze_results_quality(results)\n",
    "    \n",
    "    # –í–∏–≤–µ—Å—Ç–∏ –∑–≤—ñ—Ç\n",
    "    print_quality_report(analysis)\n",
    "    \n",
    "    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó\n",
    "    if create_visualizations and results:\n",
    "        create_quality_visualizations(analysis)\n",
    "    \n",
    "    # –ó–±–µ—Ä–µ–≥—Ç–∏ –∞–Ω–∞–ª—ñ–∑ —É JSON\n",
    "    analysis_filename = f\"{RESULTS_DIR}/quality_analysis_{RUN_ID}.json\"\n",
    "    with open(analysis_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüíæ Quality analysis saved: {analysis_filename}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def sample_manual_verification(results, sample_size=10):\n",
    "    \"\"\"–í–∏–±–∏—Ä–∞—î –≤–∏–ø–∞–¥–∫–æ–≤—É –≤–∏–±—ñ—Ä–∫—É –¥–ª—è —Ä—É—á–Ω–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è  No results for manual verification\")\n",
    "        return []\n",
    "    \n",
    "    # –í–∏–ø–∞–¥–∫–æ–≤–∞ –≤–∏–±—ñ—Ä–∫–∞\n",
    "    import random\n",
    "    sample_size = min(sample_size, len(results))\n",
    "    sample = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"\\nüîç MANUAL VERIFICATION SAMPLE ({sample_size} items)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, result in enumerate(sample, 1):\n",
    "        data = result['data']\n",
    "        print(f\"\\n{i}. {data.get('ingredient_name_uk', 'Unknown')} ({data.get('ingredient_name_lat', 'Unknown')})\")\n",
    "        print(f\"   Status: {data.get('result_status', 'unknown')}\")\n",
    "        print(f\"   Citations: {len(data.get('citations', []))}\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–µ—Ä—à—É —Ü–∏—Ç–∞—Ç—É\n",
    "        citations = data.get('citations', [])\n",
    "        if citations:\n",
    "            first_citation = citations[0]\n",
    "            print(f\"   Sample citation: {first_citation.get('title', 'No title')[:50]}...\")\n",
    "            print(f\"   URL: {first_citation.get('url', 'No URL')}\")\n",
    "            print(f\"   Verified: {first_citation.get('verified', False)}\")\n",
    "    \n",
    "    return sample\n",
    "\n",
    "print(\"üìä Quality assurance functions ready\")\n",
    "print(\"üîç Use generate_quality_report(results) for full analysis\")\n",
    "print(\"üéØ Use sample_manual_verification(results, 10) for manual checking\")\n",
    "print(\"‚úÖ Cell 6 completed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ DLSD EVIDENCE COLLECTOR NOTEBOOK READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìã Usage:\")\n",
    "print(\"1. Run cells 0-2 to set up APIs and load data\")\n",
    "print(\"2. Use run_test_batch(5) to test with 5 ingredients\")\n",
    "print(\"3. Use run_full_batch_processing() for full processing\")\n",
    "print(\"4. Use export_all_formats(results) to save results\")\n",
    "print(\"5. Use generate_quality_report(results) for analysis\")\n",
    "print(\"\\nüöÄ Ready to process 6500+ ingredients!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}