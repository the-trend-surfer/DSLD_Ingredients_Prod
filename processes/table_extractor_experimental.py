"""
EXPERIMENTAL Table Extractor - Gemini Google Search —è–∫ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç #1
"""
import json
import requests
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup
import time

from modules.multi_ai_client import multi_ai_client
from modules.ncbi_client import ncbi_client
from modules.gemini_google_search import gemini_google_searcher
from config import Config
from processes.ai_prompts import TablePrompts
from processes.table_schema import TABLE_OUTPUT_SCHEMA
from processes.source_policy import source_policy

class ExperimentalTableExtractor:
    """EXPERIMENTAL Extractor –∑ Gemini Google Search —è–∫ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º"""

    def __init__(self):
        self.system_prompt = self._get_system_prompt()

    def _get_system_prompt(self) -> str:
        return """–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ —Ö–∞—Ä—á–æ–≤–∏—Ö –¥–æ–±–∞–≤–æ–∫. –í–∏—Ç—è–≥—É–π —Ç–æ—á–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ñ –∑ 5 —Å—Ç–æ–≤–ø—á–∏–∫—ñ–≤:

1. –ù–∞–∑–≤–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é [–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ –Ω–∞–∑–≤–∞]
2. –î–∂–µ—Ä–µ–ª–æ —Å–∏—Ä–æ–≤–∏–Ω–∏ (—á–∞—Å—Ç–∏–Ω–∞ —Ä–æ—Å–ª–∏–Ω–∏/–æ—Ä–≥–∞–Ω—ñ–∑–º—É)
3. –ê–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏ (—Ç—ñ–ª—å–∫–∏ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω—ñ)
4. –î–æ–±–æ–≤–∞ –Ω–æ—Ä–º–∞ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –∞–±–æ –ø–æ—Ä–æ–∂–Ω—î –ø–æ–ª–µ)
5. –î–∂–µ—Ä–µ–ª–∞ —Ç–∞ —Ü–∏—Ç–∞—Ç–∏

–ü–æ–≤–µ—Ä—Ç–∞–π —Ç—ñ–ª—å–∫–∏ JSON –∑–≥—ñ–¥–Ω–æ —Å—Ö–µ–º–∏."""

    def extract_for_table_experimental(self, normalized_data: Dict[str, Any], accepted_sources: List[Dict[str, Any]], synonyms: Optional[List[str]] = None, existing_links: Optional[List[str]] = None, ai_model: Optional[str] = None) -> Dict[str, Any]:
        """
        –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–õ–¨–ù–ê –í–ï–†–°–Ü–Ø –∑ –ü–†–ê–í–ò–õ–¨–ù–û–Æ –ü–†–Ü–û–†–ò–¢–ò–ó–ê–¶–Ü–Ñ–Æ:
        1. PubMed/NCBI (L1)
        2. –ü—Ä—è–º—ñ L1 –¥–∂–µ—Ä–µ–ª–∞
        3. L2 –¥–∂–µ—Ä–µ–ª–∞
        4. Gemini Google Search –∑ L1-L4 —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é
        5. –Ü–Ω—à—ñ –¥–∂–µ—Ä–µ–ª–∞
        """
        try:
            # –í–∏—Ç—è–≥—É—î–º–æ –Ω–∞–∑–≤—É —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∞ –∑ –±—É–¥—å-—è–∫–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É
            if isinstance(normalized_data, dict):
                ingredient = normalized_data.get("ingredient") or normalized_data.get("name") or str(normalized_data)
            else:
                ingredient = str(normalized_data)

            print(f"[EXPERIMENTAL] Extracting table data for {ingredient}...")

            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–ø–æ—Ä–æ–∂–Ω—ñ–π –Ω–∞ –ø–æ—á–∞—Ç–∫—É)
            final_result = self._create_empty_table_result(ingredient)
            sources_tried = []

            # üîó –ï–¢–ê–ü 1: EXISTING LINKS (—Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞)
            if existing_links:
                print(f"[STAGE-1] Processing {len(existing_links)} existing links...")
                documents = []
                for link in existing_links:
                    doc = self._download_single_document(link)
                    if doc and self._is_relevant_content(doc.get('text', ''), ingredient):
                        documents.append(doc)

                if documents:
                    print(f"[EXISTING-DATA] Found {len(documents)} relevant existing documents")
                    existing_table = self._extract_with_table_ai(ingredient, documents, ai_model)
                    if existing_table:
                        final_result = self._merge_table_results(final_result, existing_table, "Existing Links")
                        sources_tried.append("Existing Links")

            # üìö –ï–¢–ê–ü 2: PUBMED/NCBI –ü–ï–†–®–ò–ô! (L1 - –Ω–∞–π–≤–∏—â–∞ —è–∫—ñ—Å—Ç—å)
            print(f"[STAGE-2] Searching PubMed/NCBI (L1 priority)...")
            all_names = [ingredient]
            if synonyms:
                all_names.extend(synonyms[:2])

            ncbi_articles = self._get_ncbi_articles(all_names)
            if ncbi_articles:
                print(f"[PUBMED-L1] Found {len(ncbi_articles)} PubMed articles")
                ncbi_table = self._extract_with_table_ai(ingredient, ncbi_articles[:3], ai_model)
                if ncbi_table:
                    final_result = self._merge_table_results(final_result, ncbi_table, "PubMed L1")
                    sources_tried.append("PubMed L1")

            # üèõÔ∏è –ï–¢–ê–ü 3: –ü–†–Ø–ú–Ü L1 –î–ñ–ï–†–ï–õ–ê (NIH, EFSA, FDA)
            print(f"[STAGE-3] Checking direct L1 sources...")
            l1_direct_results = self._check_direct_l1_sources(ingredient, synonyms, ai_model)
            if l1_direct_results:
                final_result = self._merge_table_results(final_result, l1_direct_results, "Direct L1")
                sources_tried.append("Direct L1")

            # üî¨ –ï–¢–ê–ü 4: L2 –î–ñ–ï–†–ï–õ–ê (Nature, ScienceDirect)
            print(f"[STAGE-4] Checking L2 academic sources...")
            l2_results = self._check_l2_sources(ingredient, synonyms, ai_model)
            if l2_results:
                final_result = self._merge_table_results(final_result, l2_results, "L2 Academic")
                sources_tried.append("L2 Academic")

            # üîç –ï–¢–ê–ü 5: –ü–ï–†–ï–í–Ü–†–ö–ê –ù–ï–û–ë–•–Ü–î–ù–û–°–¢–Ü GEMINI
            completion_stats = self._calculate_completion_stats(final_result)
            print(f"[COMPLETION-CHECK] Current completion: {completion_stats['percentage']:.1f}%")

            # –Ø–∫—â–æ –¥–∞–Ω—ñ –Ω–µ–ø–æ–≤–Ω—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¶–Ü–õ–¨–û–í–ò–ô Gemini –ø–æ—à—É–∫ –∑ site: –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏
            if completion_stats['percentage'] < 30:  # –Ø–∫—â–æ –º–µ–Ω—à–µ 30% –∑–∞–ø–æ–≤–Ω–µ–Ω–æ
                print(f"[STAGE-5] Data incomplete ({completion_stats['percentage']:.1f}%), using TARGETED Gemini search...")

                try:
                    # PHASE 1: –¶—ñ–ª—å–æ–≤–∏–π –ø–æ—à—É–∫ –ø–æ L1-L4 —Å–∞–π—Ç–∞—Ö –∑ site: –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏
                    missing_fields = self._check_missing_critical_fields(final_result)
                    print(f"[TARGETED] Missing critical fields: {missing_fields}")

                    google_result = self._targeted_gemini_search_with_sites(ingredient, synonyms, missing_fields)

                    if google_result and google_result.get('search_method') != 'gemini_google_search_failed':
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –∑–Ω–∞–π—à–ª–∏ –∫—Ä–∏—Ç–∏—á–Ω—ñ –ø–æ–ª—è
                        filtered_google_data = self._filter_google_results_by_l1_l4(google_result, ingredient)

                        if filtered_google_data:
                            final_result = self._merge_table_results(final_result, filtered_google_data, "Gemini Targeted")
                            sources_tried.append("Gemini Targeted")
                            print(f"[GEMINI-SUPPLEMENT] Added targeted Gemini data")
                        else:
                            print(f"[GEMINI-FILTERED] No L1-L4 sources found in Gemini results")
                    else:
                        print(f"[GEMINI-FAILED] Google Search failed or returned no results")

                except Exception as e:
                    print(f"[GEMINI-ERROR] Google Search error: {e}")
            else:
                print(f"[GEMINI-SKIP] Data complete enough ({completion_stats['percentage']:.1f}% >= 30%), skipping Gemini")

            # üîç –ï–¢–ê–ü 6: –ü–ï–†–ï–í–Ü–†–ö–ê –ü–†–û–ü–£–©–ï–ù–ò–• –ö–û–õ–û–ù–û–ö
            missing_fields = self._check_missing_fields(final_result)
            if missing_fields:
                print(f"[STAGE-6] Missing fields: {missing_fields}, trying additional sources...")
                additional_result = self._fill_missing_fields(ingredient, missing_fields, synonyms, ai_model)
                if additional_result:
                    final_result = self._merge_table_results(final_result, additional_result, "Additional Sources")
                    sources_tried.append("Additional Sources")

            # üìä –§–Ü–ù–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê
            final_completion_stats = self._calculate_completion_stats(final_result)
            print(f"[FINAL-STATS] {final_completion_stats['percentage']:.1f}% complete - Sources used: {', '.join(sources_tried) if sources_tried else 'None'}")

            return final_result

        except Exception as e:
            print(f"[ERROR] Experimental extraction failed for {ingredient}: {e}")
            return self._create_empty_table_result(ingredient)

    def _merge_table_results(self, current_result: Dict[str, Any], new_result: Dict[str, Any], source_name: str) -> Dict[str, Any]:
        """–û–±'—î–¥–Ω—É—î –¥–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–∞–±–ª–∏—Ü—ñ, –∑–∞–ø–æ–≤–Ω—é—é—á–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ñ –ø–æ–ª—è"""
        try:
            merged = current_result.copy()

            # 1. –ù–∞–∑–≤–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é [–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ –Ω–∞–∑–≤–∞]
            if not merged.get('nazva_ukr_orig') and new_result.get('nazva_ukr_orig'):
                merged['nazva_ukr_orig'] = new_result['nazva_ukr_orig']
                print(f"[MERGE] Added name from {source_name}")

            # 2. –î–∂–µ—Ä–µ–ª–æ —Å–∏—Ä–æ–≤–∏–Ω–∏
            if not merged.get('dzherelo_syrovyny') and new_result.get('dzherelo_syrovyny'):
                merged['dzherelo_syrovyny'] = new_result['dzherelo_syrovyny']
                print(f"[MERGE] Added source material from {source_name}")

            # 3. –ê–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏ (–æ–±'—î–¥–Ω—É—î–º–æ, –≤–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏)
            current_compounds = merged.get('aktyvni_spoluky', [])
            new_compounds = new_result.get('aktyvni_spoluky', [])

            if new_compounds:
                # –û–±'—î–¥–Ω—É—î–º–æ —Ç–∞ –≤–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏ (–ø–æ—Ä—ñ–≤–Ω—é—î–º–æ –Ω–∏–∂–Ω—ñ–º —Ä–µ–≥—ñ—Å—Ç—Ä–æ–º)
                combined_compounds = current_compounds.copy()
                current_lower = [c.lower() for c in current_compounds]

                for compound in new_compounds:
                    if compound.lower() not in current_lower:
                        combined_compounds.append(compound)
                        current_lower.append(compound.lower())

                if len(combined_compounds) > len(current_compounds):
                    merged['aktyvni_spoluky'] = combined_compounds[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 —Å–ø–æ–ª—É–∫
                    print(f"[MERGE] Added {len(combined_compounds) - len(current_compounds)} compounds from {source_name}")

            # 4. –î–æ–±–æ–≤–∞ –Ω–æ—Ä–º–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–µ–ø–æ—Ä–æ–∂–Ω—ñ–º –∑–Ω–∞—á–µ–Ω–Ω—è–º)
            if not merged.get('dobova_norma') and new_result.get('dobova_norma'):
                merged['dobova_norma'] = new_result['dobova_norma']
                print(f"[MERGE] Added dosage from {source_name}")

            # 5. –î–∂–µ—Ä–µ–ª–∞ —Ç–∞ —Ü–∏—Ç–∞—Ç–∏ (–æ–±'—î–¥–Ω—É—î–º–æ)
            current_citations = merged.get('dzherela_tsytaty', [])
            new_citations = new_result.get('dzherela_tsytaty', [])

            if new_citations:
                # –û–±'—î–¥–Ω—É—î–º–æ —Ü–∏—Ç–∞—Ç–∏, –ø–µ—Ä–µ–≤—ñ—Ä—è—é—á–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞ URL
                combined_citations = current_citations.copy()
                current_urls = [c.get('url', '') for c in current_citations]

                for citation in new_citations:
                    if citation.get('url', '') not in current_urls:
                        combined_citations.append(citation)
                        current_urls.append(citation.get('url', ''))

                if len(combined_citations) > len(current_citations):
                    merged['dzherela_tsytaty'] = combined_citations[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 —Ü–∏—Ç–∞—Ç
                    print(f"[MERGE] Added {len(combined_citations) - len(current_citations)} citations from {source_name}")

            return merged

        except Exception as e:
            print(f"[MERGE-ERROR] Failed to merge results from {source_name}: {e}")
            return current_result

    def _check_missing_fields(self, result: Dict[str, Any]) -> List[str]:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —è–∫—ñ –ø–æ–ª—è –≤—ñ–¥—Å—É—Ç–Ω—ñ –∞–±–æ –ø–æ—Ä–æ–∂–Ω—ñ"""
        missing = []

        if not result.get('nazva_ukr_orig'):
            missing.append('name')

        if not result.get('dzherelo_syrovyny'):
            missing.append('source_material')

        if not result.get('aktyvni_spoluky') or len(result.get('aktyvni_spoluky', [])) == 0:
            missing.append('compounds')

        if not result.get('dobova_norma'):
            missing.append('dosage')

        if not result.get('dzherela_tsytaty') or len(result.get('dzherela_tsytaty', [])) == 0:
            missing.append('citations')

        return missing

    def _calculate_completion_stats(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—á–∏—Å–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–ø–æ–≤–Ω–µ–Ω–æ—Å—Ç—ñ –ø–æ–ª—ñ–≤"""
        total_fields = 5
        filled_fields = 0

        if result.get('nazva_ukr_orig'):
            filled_fields += 1

        if result.get('dzherelo_syrovyny'):
            filled_fields += 1

        if result.get('aktyvni_spoluky') and len(result.get('aktyvni_spoluky', [])) > 0:
            filled_fields += 1

        if result.get('dobova_norma'):
            filled_fields += 1

        if result.get('dzherela_tsytaty') and len(result.get('dzherela_tsytaty', [])) > 0:
            filled_fields += 1

        percentage = (filled_fields / total_fields) * 100

        return {
            'filled_fields': filled_fields,
            'total_fields': total_fields,
            'percentage': percentage,
            'missing_fields': self._check_missing_fields(result)
        }

    def _fill_missing_fields(self, ingredient: str, missing_fields: List[str], synonyms: Optional[List[str]], ai_model: Optional[str]) -> Dict[str, Any]:
        """–ù–∞–º–∞–≥–∞—î—Ç—å—Å—è –∑–∞–ø–æ–≤–Ω–∏—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ñ –ø–æ–ª—è –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –¥–∂–µ—Ä–µ–ª"""
        try:
            print(f"[ADDITIONAL] Trying to fill missing: {', '.join(missing_fields)}")

            # –î–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –ø–æ–ª—ñ–≤ –º–æ–∂–µ–º–æ —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏:
            # 1. Wikipedia (—á–∞—Å—Ç–æ –º–∞—î –±–∞–∑–æ–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é)
            # 2. –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∑–∞–ø–∏—Ç–∏ NCBI
            # 3. –ó–∞–≥–∞–ª—å–Ω—ñ –µ–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—á–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞

            additional_sources = []

            # Wikipedia —è–∫ —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–µ –¥–∂–µ—Ä–µ–ª–æ
            if 'name' in missing_fields or 'source_material' in missing_fields:
                wiki_urls = [
                    f"https://en.wikipedia.org/wiki/{ingredient.replace(' ', '_')}",
                    f"https://uk.wikipedia.org/wiki/{ingredient.replace(' ', '_')}"
                ]

                for url in wiki_urls:
                    doc = self._download_single_document(url)
                    if doc and self._is_relevant_content(doc.get('text', ''), ingredient):
                        additional_sources.append(doc)
                        break  # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω—É Wikipedia —Å—Ç–∞—Ç—Ç—é

            # –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ NCBI –∑–∞–ø–∏—Ç–∏ –¥–ª—è —Å–ø–æ–ª—É–∫ —ñ –¥–æ–∑—É–≤–∞–Ω–Ω—è
            if 'compounds' in missing_fields or 'dosage' in missing_fields:
                specialized_queries = [
                    f'"{ingredient}" AND (chemical composition OR active compounds)',
                    f'"{ingredient}" AND (dosage OR dose OR recommended amount)'
                ]

                for query in specialized_queries:
                    try:
                        pubmed_ids = ncbi_client.search_pubmed(query, max_results=2)
                        if pubmed_ids:
                            for pmid in pubmed_ids:
                                article_data = ncbi_client.fetch_article_details(pmid)
                                if article_data:
                                    additional_sources.append({
                                        'url': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                                        'text': article_data.get('abstract', ''),
                                        'title': article_data.get('title', '')
                                    })
                                    break  # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω—É —Å—Ç–∞—Ç—Ç—é –Ω–∞ –∑–∞–ø–∏—Ç
                    except Exception as e:
                        print(f"[ADDITIONAL-NCBI] Specialized query failed: {e}")
                        continue

            # –û–±—Ä–æ–±–ª—è—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –¥–∂–µ—Ä–µ–ª–∞
            if additional_sources:
                print(f"[ADDITIONAL] Found {len(additional_sources)} additional sources")
                return self._extract_with_table_ai(ingredient, additional_sources[:2], ai_model)

            return None

        except Exception as e:
            print(f"[ADDITIONAL-ERROR] Failed to fill missing fields: {e}")
            return None

    def _convert_google_result_to_table_ukrainian(self, google_result: Dict[str, Any], ingredient: str) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç Google Search –≤ —Ñ–æ—Ä–º–∞—Ç —Ç–∞–±–ª–∏—Ü—ñ –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é —á–µ—Ä–µ–∑ AI"""
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ AI –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É —Ç–∞ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è
            raw_response = google_result.get('raw_response', '')
            google_sources = google_result.get('google_sources', [])

            if not raw_response:
                return self._create_empty_table_result(ingredient)

            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—É –∑ –¥–∂–µ—Ä–µ–ª –¥–ª—è –∫—Ä–∞—â–∏—Ö —Ü–∏—Ç–∞—Ç
            sources_content = ""
            valid_sources = []

            print(f"[DEBUG] Processing {len(google_sources)} Google sources for citations")

            # Gemini –Ω–µ –ø–æ–≤–µ—Ä—Ç–∞—î content —É google_sources, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ raw_response
            if raw_response and len(raw_response) > 100:
                print(f"[DEBUG] Using Gemini raw_response as content source: {len(raw_response)} chars")
                sources_content = f"\n–ì–ª–∞–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç Gemini Google Search:\n{raw_response[:1000]}\n"

                # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∂–µ—Ä–µ–ª–∞ –∑ –∫–æ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–∏—Ö URLs
                for i, source in enumerate(google_sources[:3]):
                    url = source.get('url', '')
                    title = source.get('title', '')

                    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ VertexAI URL –≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—é —Å—Ç–æ—Ä—ñ–Ω–∫–æ—é
                    converted_url = self._convert_vertexai_to_real_url(url, title, ingredient)

                    valid_sources.append({
                        'url': converted_url,
                        'content': f"Content from Gemini analysis: {title}",
                        'title': title
                    })
                    print(f"[DEBUG] Added Google source: {title} -> {converted_url}")
            else:
                print(f"[DEBUG] No useful raw_response from Gemini: {len(raw_response)} chars")

            print(f"[DEBUG] Found {len(valid_sources)} Google sources")

            # –°—Ç–≤–æ—Ä—é—î–º–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–æ–±–∫–∏ Google —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            ukrainian_prompt = f"""–û–±—Ä–æ–±—ñ –¥–∞–Ω—ñ –ø—Ä–æ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç {ingredient} –∑ Google Search —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤. –û–ë–û–í'–Ø–ó–ö–û–í–û —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é.

–í–ò–•–Ü–î–ù–Ü –î–ê–ù–Ü –ó GOOGLE:
{raw_response[:1000]}

–î–û–î–ê–¢–ö–û–í–ò–ô –ö–û–ù–¢–ï–ù–¢ –ó –î–ñ–ï–†–ï–õ:
{sources_content[:1000]}

‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û –î–õ–Ø –¶–ò–¢–ê–¢:
‚ùå –ó–ê–ë–û–†–û–ù–ï–ù–û –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞–∑–≤–∏ –¥–æ–º–µ–Ω—ñ–≤ —è–∫ —Ü–∏—Ç–∞—Ç–∏: "ahcc.net", "wikipedia.org", "pubmed.ncbi.nlm.nih.gov"
‚ùå –ó–ê–ë–û–†–û–ù–ï–ù–û –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞–∑–≤–∏ —Å–∞–π—Ç—ñ–≤, URL –∞–±–æ –¥–æ–º–µ–Ω–∏ –≤ —è–∫–æ—Å—Ç—ñ —Ü–∏—Ç–∞—Ç
‚ùå –ó–ê–ë–û–†–û–ù–ï–ù–û –¥–æ–¥–∞–≤–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≤—ñ—Ç–∞–º—ñ–Ω D, –º—ñ–Ω–µ—Ä–∞–ª–∏ –∞–±–æ —ñ–Ω—à—ñ —Ä–µ—á–æ–≤–∏–Ω–∏ –ù–ï –ø–æ–≤'—è–∑–∞–Ω—ñ –∑ {ingredient}
‚úÖ –û–ë–û–í'–Ø–ó–ö–û–í–û –≤–∏—Ç—è–≥—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –†–ï–ß–ï–ù–ù–Ø –∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–∂–µ—Ä–µ–ª –≤–∏—â–µ
‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ç—ñ–ª—å–∫–∏ —Ñ–∞–∫—Ç–∏—á–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ {ingredient}
‚úÖ –¶–∏—Ç–∞—Ç–∏ –º–∞—é—Ç—å –±—É—Ç–∏ —Ä–µ–∞–ª—å–Ω–∏–º–∏ —Ä–µ—á–µ–Ω–Ω—è–º–∏ –∑ —Ç–µ–∫—Å—Ç—É, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "AHCC shows immunomodulatory effects in clinical studies"
‚úÖ –Ø–∫—â–æ –Ω–µ–º–∞—î —Ö–æ—Ä–æ—à–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É - –∫—Ä–∞—â–µ –∑–∞–ª–∏—à–∏—Ç–∏ —Ü–∏—Ç–∞—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º–∏

–ó–ê–í–î–ê–ù–ù–Ø - —Å—Ç–≤–æ—Ä–∏ —Ç–∞–±–ª–∏—Ü—é –∑ 5 —Å—Ç–æ–≤–ø—á–∏–∫—ñ–≤ —É –¢–û–ß–ù–û–ú–£ —Ñ–æ—Ä–º–∞—Ç—ñ:

1. –ù–∞–∑–≤–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é [–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ –Ω–∞–∑–≤–∞] - –ø—Ä–∏–∫–ª–∞–¥–∏: "Œ≤-—Å–∏—Ç–æ—Å—Ç–µ—Ä–∏–Ω (Œ≤-Sitosterol)", "–õ–µ—Ü–∏—Ç–∏–Ω (—Ñ–æ—Å—Ñ–∞—Ç–∏–¥—ñ–ª—Ö–æ–ª—ñ–Ω) (Lecithin)"

2. –î–∂–µ—Ä–µ–ª–æ —Å–∏—Ä–æ–≤–∏–Ω–∏ - –¢–û–ß–ù–ò–ô —Ñ–æ—Ä–º–∞—Ç —è–∫ —É –ø—Ä–∏–∫–ª–∞–¥–∞—Ö:
   ‚úÖ "–°–æ—è, –º–æ—Ä–∫–≤–∞, —ñ–Ω–∂–∏—Ä, –∫–æ—Ä—ñ–∞–Ω–¥—Ä, –¥—è–≥–µ–ª—å –ª—ñ–∫–∞—Ä—Å—å–∫–∏–π –∫–æ—Ä–µ–Ω—ñ, –ø–ª–æ–¥–∏ (Angelica archangelica)"
   ‚úÖ "–û–ª—ñ—ó —Ä–æ—Å–ª–∏–Ω–Ω—ñ (—Å–æ—î–≤–∞, –æ–ª–∏–≤–∫–æ–≤–∞, –∑–∞—Ä–æ–¥–∫—ñ–≤ –ø—à–µ–Ω–∏—Ü—ñ), –ú–∞–∫–∞ –ø–µ—Ä—É–∞–Ω—Å—å–∫–∞ –∫–æ—Ä–µ–Ω–µ–ø–ª–æ–¥–∏ (Lepidium meyenii)"
   ‚úÖ "–°—É–±–ø—Ä–æ–¥—É–∫—Ç–∏ —Ç–≤–∞—Ä–∏–Ω–Ω–æ–≥–æ –ø–æ—Ö–æ–¥–∂–µ–Ω–Ω—è (–ø—Ä–æ–¥—É–∫—Ç –≥—ñ–¥—Ä–æ–ª—ñ–∑—É —Ö—Ä—è—â–æ–≤–æ—ó —Ç–∫–∞–Ω–∏–Ω–∏ –ø—Ç–∏—Ü—ñ, —Ç–≤–∞—Ä–∏–Ω, –º–æ—Ä—Å—å–∫–∏—Ö –æ—Ä–≥–∞–Ω—ñ–∑–º—ñ–≤)"
   ‚úÖ "–ú–æ–ª–æ–∫–æ, —Ä–∏–±–∞, –º'—è—Å–æ (–∞–±–æ –æ—Ç—Ä–∏–º–∞–Ω–∏–π —à–ª—è—Ö–æ–º –±—ñ–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑—É)"

3. –ê–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏ - —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –≤ –¥—É–∂–∫–∞—Ö: "–ì—ñ–∞–ª—É—Ä–æ–Ω–æ–≤–∞ –∫–∏—Å–ª–æ—Ç–∞ (Hyaluronic acid)"

4. –î–æ–±–æ–≤–∞ –Ω–æ—Ä–º–∞ - —Ç—ñ–ª—å–∫–∏ —Ü–∏—Ñ—Ä–∏ —Ç–∞ –æ–¥–∏–Ω–∏—Ü—ñ –∑ —Ç–µ–∫—Å—Ç—É

5. –î–∂–µ—Ä–µ–ª–∞ —Ç–∞ —Ü–∏—Ç–∞—Ç–∏ - —Ç–æ—á–Ω—ñ —Ü–∏—Ç–∞—Ç–∏ –∑ —Ç–µ–∫—Å—Ç—É

–û–ë–û–í'–Ø–ó–ö–û–í–Ü –ü–†–ê–í–ò–õ–ê –§–û–†–ú–ê–¢–£:
‚úÖ –£–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –Ω–∞–∑–≤–∏ —ó–∂—ñ: —Å–æ—è, –º–æ—Ä–∫–≤–∞, –ø–æ–º—ñ–¥–æ—Ä–∏, —è–±–ª—É–∫–∞ (–ù–ï "—Å–æ—î–≤—ñ –±–æ–±–∏")
‚úÖ –ß–∞—Å—Ç–∏–Ω–∏ —Ä–æ—Å–ª–∏–Ω: –∫–æ—Ä–µ–Ω—ñ, –ª–∏—Å—Ç—è, –∫–≤—ñ—Ç–∫–∏, –ø–ª–æ–¥–∏, –Ω–∞—Å—ñ–Ω–Ω—è
‚úÖ –õ–∞—Ç–∏–Ω—Å—å–∫—ñ –Ω–∞–∑–≤–∏ –≤ –¥—É–∂–∫–∞—Ö: (Angelica archangelica), (Lepidium meyenii)
‚úÖ –û–±—Ä–æ–±–∫–∞: –æ–ª—ñ—ó —Ä–æ—Å–ª–∏–Ω–Ω—ñ, –ø—Ä–æ–¥—É–∫—Ç –≥—ñ–¥—Ä–æ–ª—ñ–∑—É, –±—ñ–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏–π —Å–∏–Ω—Ç–µ–∑
‚úÖ –î–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö: "–∞–±–æ –æ—Ç—Ä–∏–º–∞–Ω–∏–π —à–ª—è—Ö–æ–º –±—ñ–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑—É"
‚úÖ –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∫–æ–º–∞–º–∏ –º—ñ–∂ –¥–∂–µ—Ä–µ–ª–∞–º–∏

JSON —Ñ–æ—Ä–º–∞—Ç:
{{
  "nazva_ukr_orig": "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –Ω–∞–∑–≤–∞ [Original Name]",
  "dzherelo_syrovyny": "—É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π –æ—Ä–≥–∞–Ω—ñ–∑–º; —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ —á–∞—Å—Ç–∏–Ω–∞",
  "aktyvni_spoluky": ["—É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –Ω–∞–∑–≤–∏ —Å–ø–æ–ª—É–∫"],
  "dobova_norma": "–¥–æ–∑—É–≤–∞–Ω–Ω—è" –∞–±–æ "",
  "dzherela_tsytaty": [...]
}}

–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò JSON."""

            # –í–∏–∫–ª–∏–∫–∞—î–º–æ AI –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏
            response = self._call_ai_direct(ukrainian_prompt, "openai")  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ OpenAI –¥–ª—è –ø–æ—à–∏—Ä–µ–Ω–∏—Ö —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –Ω–∞–∑–≤

            if response and '{' in response:
                data = self._extract_json_from_response(response)
                if data and self._validate_table_data(data):
                    print(f"[OK] AI generated Ukrainian data for {ingredient}")

                    # –ü–û–í–ù–Ü–°–¢–Æ –Ü–ì–ù–û–†–£–Ñ–ú–û AI —Ü–∏—Ç–∞—Ç–∏ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¢–Ü–õ–¨–ö–ò Gemini raw_response!
                    print(f"[CITATIONS] Ignoring AI citations, using ONLY Gemini raw_response")
                    improved_citations = []

                    # –í–∏—Ç—è–≥—É—î–º–æ —Ä–µ–∞–ª—å–Ω—ñ —Ü–∏—Ç–∞—Ç–∏ –∑ Gemini response
                    real_quotes = self._extract_real_quotes_from_response(raw_response, 0)
                    print(f"[CITATIONS] Extracted {len(real_quotes)} real quotes from Gemini")

                    for i, real_quote in enumerate(real_quotes[:3]):
                        if len(real_quote) > 30:
                            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–µ –¥–∂–µ—Ä–µ–ª–æ –∑ —Ä–µ–∞–ª—å–Ω–∏–º URL
                            source_url = ""
                            source_title = ""
                            if i < len(valid_sources):
                                source = valid_sources[i]
                                original_url = source.get('url', '')
                                source_title = source.get('title', '')
                                # –î–µ–∫–æ–¥—É—î–º–æ —Ä–µ–∞–ª—å–Ω–∏–π URL
                                real_url = self._convert_vertexai_to_real_url(original_url, source_title, ingredient)
                                source_url = real_url if real_url else original_url
                            else:
                                source_url = "https://scholar.google.com"  # L1 –¥–∂–µ—Ä–µ–ª–æ –¥–ª—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö —Ü–∏—Ç–∞—Ç

                            improved_citations.append({
                                'url': source_url,
                                'quote': real_quote,
                                'type': 'Google Search - Gemini real content'
                            })
                            print(f"[OK] Citation {i+1}: {real_quote[:50]}... | URL: {source_url[:40]}...")

                    # –Ø–∫—â–æ –º–∞–ª–æ —Ü–∏—Ç–∞—Ç –∑ Gemini - –¥–æ–¥–∞—î–º–æ –∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–∂–µ—Ä–µ–ª
                    if len(improved_citations) < 2 and valid_sources:
                        print(f"[CITATIONS] Adding backup citations from source content")
                        for source in valid_sources[:3-len(improved_citations)]:
                            content_text = source.get('content', '').strip()
                            if len(content_text) > 50 and not content_text.endswith('.com'):
                                # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à—É –∑–º—ñ—Å—Ç–æ–≤–Ω—É —á–∞—Å—Ç–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç—É —è–∫ —Ü–∏—Ç–∞—Ç—É
                                quote_text = content_text[:150] + "..." if len(content_text) > 150 else content_text
                                real_url = self._convert_vertexai_to_real_url(source.get('url', ''), source.get('title', ''), ingredient)

                                improved_citations.append({
                                    'url': real_url if real_url else source.get('url', ''),
                                    'quote': quote_text,
                                    'type': 'Google Search - source content extract'
                                })
                                print(f"[OK] Backup citation: {quote_text[:50]}...")

                    data['dzherela_tsytaty'] = improved_citations
                    print(f"[FINAL] Total real citations: {len(improved_citations)}")

                    return data
                else:
                    print(f"[ERROR] AI response validation failed for {ingredient}")
            else:
                print(f"[ERROR] No valid JSON in AI response for {ingredient}")

            return self._create_empty_table_result(ingredient)

        except Exception as e:
            print(f"[ERROR] Ukrainian Google result conversion failed: {e}")
            return self._create_empty_table_result(ingredient)

    def _convert_google_result_to_table(self, google_result: Dict[str, Any], ingredient: str) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç Google Search –≤ —Ñ–æ—Ä–º–∞—Ç —Ç–∞–±–ª–∏—Ü—ñ"""
        try:
            # –í–∏—Ç—è–≥—É—î–º–æ –¥–∞–Ω—ñ –∑ Google —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
            source_material = google_result.get('source_material', {})
            active_compounds = google_result.get('active_compounds', [])
            dosage_info = google_result.get('dosage_info', {})
            google_sources = google_result.get('google_sources', [])

            # –§–æ—Ä–º—É—î–º–æ –Ω–∞–∑–≤—É (—Ç—ñ–ª—å–∫–∏ –∑ —Ä–µ–∞–ª—å–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏)
            scientific_name = source_material.get('scientific_name', '')
            if scientific_name and scientific_name not in ['scientific name', 'various', 'unknown']:
                nazva = f"{ingredient} [{scientific_name}]"
            else:
                nazva = ingredient  # –ë–µ–∑ —à–∞–±–ª–æ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É

            # –§–æ—Ä–º—É—î–º–æ –¥–∂–µ—Ä–µ–ª–æ —Å–∏—Ä–æ–≤–∏–Ω–∏ (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —î –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –¥–∞–Ω—ñ)
            organism = source_material.get('organism', '')
            part_used = source_material.get('part_used', '')
            kingdom = source_material.get('kingdom', '')

            dzherelo_parts = []
            # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–º—ñ—Å—Ç–æ–≤–Ω—ñ –¥–∞–Ω—ñ
            if organism and organism not in ['organism', 'extract', 'various']:
                dzherelo_parts.append(organism)
            if part_used and part_used not in ['part', 'root', 'extract', 'various']:
                dzherelo_parts.append(part_used)

            # –Ø–∫—â–æ –Ω–µ–º–∞—î –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –∑–∞–ª–∏—à–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–º
            dzherelo_syrovyny = '; '.join(dzherelo_parts) if dzherelo_parts else ""

            # –§–æ—Ä–º—É—î–º–æ –∞–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏ (—Ç—ñ–ª—å–∫–∏ —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑ –¥–∂–µ—Ä–µ–ª)
            aktyvni_spoluky = []
            for compound in active_compounds[:5]:  # –ú–∞–∫—Å–∏–º—É–º 5 —Å–ø–æ–ª—É–∫
                compound_name = compound.get('name', '')
                concentration = compound.get('concentration', '')
                if compound_name:
                    if concentration and concentration != 'not specified':
                        aktyvni_spoluky.append(f"{compound_name} ({concentration})")
                    else:
                        aktyvni_spoluky.append(compound_name)

            # –§–æ—Ä–º—É—î–º–æ –¥–æ–∑—É–≤–∞–Ω–Ω—è
            daily_dose = dosage_info.get('daily_dose', '')
            clinical_dose = dosage_info.get('clinical_dose', '')
            dobova_norma = daily_dose or clinical_dose or ""

            # –§–æ—Ä–º—É—î–º–æ —Ü–∏—Ç–∞—Ç–∏ –∑ Google –¥–∂–µ—Ä–µ–ª
            dzherela_tsytaty = []
            for source in google_sources[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –¥–∂–µ—Ä–µ–ª–∞
                url = source.get('url')
                if url:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –¥–∂–µ—Ä–µ–ª–∞ (–∞–ª–µ –Ω–µ –≤—ñ–¥–∫–∏–¥–∞—î–º–æ)
                    url_classification = source_policy.classify_url(url)
                    priority_level = url_classification.get('source_priority', 5)  # 5 = –Ω–∞–π–Ω–∏–∂—á–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç

                    # –í–∏—Ç—è–≥—É—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É —Ü–∏—Ç–∞—Ç—É –∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É, –∞ –Ω–µ title
                    quote = ""
                    if source.get('content'):
                        # –®—É–∫–∞—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ –ø—Ä–æ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç
                        content = source['content'][:500].lower()
                        ingredient_lower = ingredient.lower()

                        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ä–µ—á–µ–Ω–Ω—è –∑ –Ω–∞–∑–≤–æ—é —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∞
                        sentences = content.split('.')
                        for sentence in sentences:
                            if ingredient_lower in sentence or any(syn.lower() in sentence for syn in (active_compounds or [])[:3]):
                                quote = sentence.strip()[:100] + "..." if len(sentence) > 100 else sentence.strip()
                                break

                    # Fallback –¥–æ title —è–∫—â–æ –Ω–µ–º–∞—î –∫–æ–Ω—Ç–µ–Ω—Ç—É
                    if not quote and source.get('title'):
                        quote = source['title'][:50] + "..." if len(source['title']) > 50 else source['title']

                    if quote:
                        dzherela_tsytaty.append({
                            "url": url,
                            "quote": quote,
                            "type": f"L{priority_level} –¥–∂–µ—Ä–µ–ª–æ"
                        })

            # –Ø–∫—â–æ –Ω–µ–º–∞—î —Ü–∏—Ç–∞—Ç –∑ Google –¥–∂–µ—Ä–µ–ª, –∑–∞–ª–∏—à–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–º
            if not dzherela_tsytaty:
                dzherela_tsytaty = []

            return {
                "nazva_ukr_orig": nazva,
                "dzherelo_syrovyny": dzherelo_syrovyny,
                "aktyvni_spoluky": aktyvni_spoluky,
                "dobova_norma": dobova_norma,
                "dzherela_tsytaty": dzherela_tsytaty
            }

        except Exception as e:
            print(f"[ERROR] Google result conversion failed: {e}")
            return self._create_empty_table_result(ingredient)

    def _download_single_document(self, url: str) -> Optional[Dict[str, Any]]:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            text_content = self._extract_text_from_html(response.text)
            if text_content and len(text_content) > 100:
                return {
                    "url": url,
                    "text": text_content[:3000]
                }
            return None
        except Exception as e:
            print(f"[ERROR] Failed to download {url}: {e}")
            return None

    def _extract_text_from_html(self, html_content: str) -> str:
        """–í–∏—Ç—è–≥—É—î —á–∏—Å—Ç–∏–π —Ç–µ–∫—Å—Ç –∑ HTML"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            return text
        except Exception as e:
            print(f"[ERROR] HTML parsing error: {e}")
            return ""

    def _is_relevant_content(self, text: str, ingredient: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π –¥–æ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∞"""
        if not text or len(text) < 200:
            return False
        text_lower = text.lower()
        ingredient_lower = ingredient.lower()
        return ingredient_lower in text_lower

    def _get_ncbi_articles(self, all_names: List[str]) -> List[Dict[str, Any]]:
        """–û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç—Ç—ñ –∑ NCBI"""
        try:
            queries = []
            for name in all_names[:2]:
                if len(name) > 2:
                    queries.extend([
                        f'"{name}"[Title/Abstract] AND (supplement OR nutrition OR dietary)',
                        f'"{name}" AND (active compound OR bioactive OR phytochemical)'
                    ])

            queries = list(set(queries[:4]))
            print(f"[NCBI] Running {len(queries)} queries...")

            articles = ncbi_client.search_multiple_queries(queries, max_per_query=2)

            documents = []
            for article in articles:
                documents.append({
                    "url": article.get("url", ""),
                    "text": article.get("abstract", "") + " " + article.get("full_text", ""),
                    "title": article.get("title", ""),
                    "pmid": article.get("pmid", "")
                })

            print(f"[NCBI] Found {len(documents)} articles with content")
            return documents

        except Exception as e:
            print(f"[ERROR] NCBI search failed: {e}")
            return []

    def _extract_with_table_ai(self, ingredient: str, documents: List[Dict[str, Any]], ai_model: Optional[str] = None) -> Dict[str, Any]:
        """–í–∏—Ç—è–≥—É—î –¥–∞–Ω—ñ —á–µ—Ä–µ–∑ AI –¥–ª—è —Ç–∞–±–ª–∏—Ü—ñ"""
        try:
            if not documents:
                return self._create_empty_table_result(ingredient)

            print(f"[INFO] Processing {len(documents)} documents for {ingredient}")

            for i, doc in enumerate(documents[:2], 1):
                url = doc.get('url', '')
                text = doc.get('text', '')
                title = doc.get('title', '')

                full_text = f"{title}\n\n{text}" if title else text

                if len(full_text) < 50:
                    continue

                print(f"[INFO] Document {i}: {len(full_text)} chars")

                prompt = TablePrompts.get_table_extraction_prompt(ingredient, full_text, url)
                response = self._call_ai_direct(prompt, ai_model)

                if response and '{' in response:
                    data = self._extract_json_from_response(response)
                    if data and self._validate_table_data(data):
                        print(f"[OK] Valid data extracted from document {i}")
                        return data

            return self._create_empty_table_result(ingredient)

        except Exception as e:
            print(f"[ERROR] Table AI extraction failed: {e}")
            return self._create_empty_table_result(ingredient)

    def _call_ai_direct(self, prompt: str, ai_model: Optional[str] = None) -> str:
        """–í–∏–∫–ª–∏–∫–∞—î AI –º–æ–¥–µ–ª—ñ"""
        try:
            if ai_model and ai_model in multi_ai_client.available_models:
                if ai_model == 'claude':
                    return multi_ai_client._call_claude(prompt)
                elif ai_model == 'openai':
                    return multi_ai_client._call_openai(prompt)
                elif ai_model == 'gemini':
                    return multi_ai_client._call_gemini(prompt)

            # Fallback
            for model_type in ['claude', 'openai', 'gemini']:
                if model_type in multi_ai_client.available_models:
                    try:
                        if model_type == 'claude':
                            return multi_ai_client._call_claude(prompt)
                        elif model_type == 'openai':
                            return multi_ai_client._call_openai(prompt)
                        elif model_type == 'gemini':
                            return multi_ai_client._call_gemini(prompt)
                    except:
                        continue
            return ""
        except Exception as e:
            print(f"[ERROR] AI call failed: {e}")
            return ""

    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """–í–∏—Ç—è–≥—É—î JSON –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ AI"""
        try:
            if '```json' in response:
                start_marker = '```json'
                end_marker = '```'
                start_idx = response.find(start_marker) + len(start_marker)
                end_idx = response.find(end_marker, start_idx)
                if start_idx != -1 and end_idx != -1:
                    json_str = response[start_idx:end_idx].strip()
                    return json.loads(json_str)

            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1

            if start_idx != -1 and end_idx != -1:
                json_str = response[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return {}

        except (json.JSONDecodeError, ValueError) as e:
            print(f"[ERROR] JSON parsing error: {e}")
            return {}

    def _validate_table_data(self, data: Dict[str, Any]) -> bool:
        """–í–∞–ª—ñ–¥—É—î –¥–∞–Ω—ñ —Ç–∞–±–ª–∏—Ü—ñ"""
        try:
            required_fields = ["nazva_ukr_orig", "dzherelo_syrovyny", "aktyvni_spoluky", "dobova_norma", "dzherela_tsytaty"]
            for field in required_fields:
                if field not in data:
                    return False
            if not isinstance(data["aktyvni_spoluky"], list):
                return False
            if not isinstance(data["dzherela_tsytaty"], list):
                return False
            return True
        except:
            return False

    def _extract_real_quotes_from_response(self, raw_response: str, existing_count: int) -> List[str]:
        """–í–∏—Ç—è–≥—É—î —Ä–µ–∞–ª—å–Ω—ñ —Ü–∏—Ç–∞—Ç–∏ –∑ Gemini raw_response"""
        quotes = []

        if not raw_response:
            return quotes

        # –†–æ–∑–¥—ñ–ª—è—î–º–æ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è
        sentences = []
        for delimiter in ['. ', '! ', '? ', '; ']:
            parts = raw_response.split(delimiter)
            for part in parts:
                clean_part = part.strip()
                if len(clean_part) > 40 and len(clean_part) < 200:
                    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —è–∫—ñ—Å–Ω—ñ —Ä–µ—á–µ–Ω–Ω—è
                    if any(keyword in clean_part.lower() for keyword in
                          ['studies', 'research', 'shown', 'found', 'demonstrated', 'clinical', 'trial', 'effect', 'dose', 'dosage', 'mg', 'supplement']):
                        sentences.append(clean_part)

        # –ë–µ—Ä–µ–º–æ –Ω–∞–π–∫—Ä–∞—â—ñ —Ä–µ—á–µ–Ω–Ω—è —è–∫ —Ü–∏—Ç–∞—Ç–∏
        needed_quotes = 3 - existing_count
        for sentence in sentences[:needed_quotes]:
            if len(sentence) > 30:
                # –û—á–∏—â—É—î–º–æ —Ä–µ—á–µ–Ω–Ω—è –≤—ñ–¥ –∑–∞–π–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤
                clean_sentence = sentence.strip('.,;:!?').strip()
                if clean_sentence and not clean_sentence.lower().startswith('source'):
                    quotes.append(clean_sentence)

        # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ—á–µ–Ω—å, –±–µ—Ä–µ–º–æ –∑–∞–≥–∞–ª—å–Ω—ñ —Ñ—Ä–∞–∑–∏
        if not quotes and needed_quotes > 0:
            general_parts = raw_response.split('\n')
            for part in general_parts:
                clean_part = part.strip()
                if 30 < len(clean_part) < 150 and not clean_part.lower().startswith(('search', 'source', 'find')):
                    quotes.append(clean_part)
                    if len(quotes) >= needed_quotes:
                        break

        return quotes[:needed_quotes]

    def _check_missing_critical_fields(self, result: Dict[str, Any]) -> List[str]:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —è–∫—ñ –∫—Ä–∏—Ç–∏—á–Ω—ñ –ø–æ–ª—è –≤—ñ–¥—Å—É—Ç–Ω—ñ"""
        missing = []

        if not result.get('dzherelo_syrovyny') or result.get('dzherelo_syrovyny') == '':
            missing.append('source_material')

        if not result.get('aktyvni_spoluky') or len(result.get('aktyvni_spoluky', [])) == 0:
            missing.append('active_compounds')

        if not result.get('dobova_norma') or result.get('dobova_norma') == '':
            missing.append('dosage')

        return missing

    def _targeted_gemini_search_with_sites(self, ingredient: str, synonyms: List[str], missing_fields: List[str]) -> Dict[str, Any]:
        """–¶—ñ–ª—å–æ–≤–∏–π Gemini –ø–æ—à—É–∫ –∑ site: –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏ –¥–ª—è L1-L4 –¥–∂–µ—Ä–µ–ª"""
        try:
            # –§–æ—Ä–º—É—î–º–æ site: –æ–ø–µ—Ä–∞—Ç–æ—Ä–∏ –¥–ª—è L1-L4 –¥–∂–µ—Ä–µ–ª
            l1_sites = [
                "site:pubmed.ncbi.nlm.nih.gov", "site:ncbi.nlm.nih.gov", "site:nih.gov",
                "site:efsa.europa.eu", "site:fda.gov",
                "site:scholar.google.com", "site:en.wikipedia.org"
            ]

            l2_sites = [
                "site:nature.com", "site:science.org", "site:sciencedirect.com"
            ]

            l3_sites = [
                "site:examine.com", "site:consumerlab.com"
            ]

            # –°–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ –Ω–∞ L1-L4 —Å–∞–π—Ç–∞—Ö
            priority_sites = " OR ".join(l1_sites + l2_sites + l3_sites)
            search_terms = [ingredient] + (synonyms[:2] if synonyms else [])
            ingredient_query = " OR ".join([f'"{term}"' for term in search_terms])

            targeted_query = f'({priority_sites}) AND ({ingredient_query})'

            print(f"[TARGETED-SEARCH] Phase 1 - L1-L4 sites for: {ingredient}")
            print(f"[TARGETED-QUERY] {targeted_query[:100]}...")

            # –§–æ—Ä–º—É—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–º–ø—Ç
            missing_info = ""
            if 'source_material' in missing_fields:
                missing_info += "- Biological source/organism (what plant, animal, bacteria it comes from)\n"
            if 'active_compounds' in missing_fields:
                missing_info += "- Active compounds/chemicals with concentrations\n"
            if 'dosage' in missing_fields:
                missing_info += "- Daily dosage recommendations from clinical studies\n"

            targeted_prompt = f"""Search for scientific data about dietary supplement: {ingredient}

SEARCH QUERY: {targeted_query}

Find these missing data points:
{missing_info}

Focus on peer-reviewed sources, clinical studies, and official health agencies.
Return structured information with exact citations and URLs."""

            result = gemini_google_searcher.search_comprehensive_ingredient_data(
                ingredient, synonyms, custom_prompt=targeted_prompt
            )

            if result and result.get('search_method') != 'gemini_google_search_failed':
                print(f"[TARGETED-SUCCESS] Found data from L1-L4 targeted search")
                return result

            # PHASE 2: –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ –Ω–∞ L1-L4, –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫
            print(f"[TARGETED-FALLBACK] Phase 2 - General search for missing fields")

            general_prompt = f"""Search for scientific data about dietary supplement: {ingredient}

SEARCH TERMS: "{ingredient}" OR "{' OR '.join(search_terms[:3])}"

Find these missing data points:
{missing_info}

Include any reliable sources with scientific backing."""

            return gemini_google_searcher.search_comprehensive_ingredient_data(
                ingredient, synonyms, custom_prompt=general_prompt
            )

        except Exception as e:
            print(f"[ERROR] Targeted Gemini search failed: {e}")
            return {}

    def _create_empty_table_result(self, ingredient: str) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä—é—î –ø–æ—Ä–æ–∂–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞–±–ª–∏—Ü—ñ"""
        return {
            "nazva_ukr_orig": "",  # –ü–æ—Ä–æ–∂–Ω—î - pipeline —Å–∞–º —Å—Ç–≤–æ—Ä–∏—Ç—å –Ω–∞–∑–≤—É
            "dzherelo_syrovyny": "",
            "aktyvni_spoluky": [],
            "dobova_norma": "",
            "dzherela_tsytaty": []
        }

    def _convert_vertexai_to_real_url(self, vertexai_url: str, title: str, ingredient: str = "") -> str:
        """–î–ï–ö–û–î–£–Ñ –†–ï–ê–õ–¨–ù–ò–ô URL –∑ VertexAI redirect —á–µ—Ä–µ–∑ HTTP requests"""
        if 'vertexaisearch.cloud.google.com' not in vertexai_url:
            return vertexai_url

        try:
            print(f"[URL-HTTP] Following redirect for: {vertexai_url[:80]}...")

            import requests
            from requests.adapters import HTTPAdapter
            from urllib3.util.retry import Retry

            # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–µ—Å—ñ—ó –∑ timeout —Ç–∞ retry
            session = requests.Session()
            retry_strategy = Retry(
                total=2,
                backoff_factor=0.5,
                status_forcelist=[429, 500, 502, 503, 504],
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session.mount("http://", adapter)
            session.mount("https://", adapter)

            # –í–∏–∫–æ–Ω—É—î–º–æ HTTP HEAD –∑–∞–ø–∏—Ç —â–æ–± —Å–ª—ñ–¥—É–≤–∞—Ç–∏ –∑–∞ redirects
            response = session.head(
                vertexai_url,
                allow_redirects=True,
                timeout=8,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
            )

            final_url = response.url

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ —Å–ø—Ä–∞–≤–¥—ñ —ñ–Ω—à–∏–π URL
            if final_url != vertexai_url and not final_url.startswith('https://vertexaisearch.cloud.google.com'):
                print(f"[URL-SUCCESS] Real URL found: {final_url}")
                return final_url
            else:
                print(f"[URL-REDIRECT-FAILED] No real redirect found")
                return self._get_real_url_from_title(title)

        except Exception as e:
            print(f"[URL-HTTP-ERROR] Request failed: {e}")
            return self._get_real_url_from_title(title)

    def _get_real_url_from_title(self, title: str) -> str:
        """–û—Ç—Ä–∏–º—É—î —Ä–µ–∞–ª—å–Ω–∏–π URL –Ω–∞ –æ—Å–Ω–æ–≤—ñ title —è–∫ –û–°–¢–ê–ù–ù–Ü–ô fallback"""
        if not title:
            print(f"[URL-FALLBACK] No title provided, returning empty")
            return ""

        title_lower = title.lower()
        print(f"[URL-FALLBACK] Trying title-based mapping for: {title}")

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –†–ï–ê–õ–¨–ù–Ü –≤—ñ–¥–æ–º—ñ URL
        real_mappings = {
            'ahcc.net': 'https://ahcc.net',
            'sourcenaturals.com': 'https://www.sourcenaturals.com',
            'qualityoflife.net': 'https://qualityoflife.net',
            'verywellhealth.com': 'https://www.verywellhealth.com',
            'examine.com': 'https://examine.com',
            'wikipedia.org': 'https://en.wikipedia.org',
            'nih.gov': 'https://www.nih.gov',
            'pubmed': 'https://pubmed.ncbi.nlm.nih.gov'
        }

        for domain, base_url in real_mappings.items():
            if domain in title_lower:
                print(f"[URL-FALLBACK] Found mapping: {title} -> {base_url}")
                return base_url

        # –Ø–∫—â–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—î –∑–∞–º—ñ—Å—Ç—å —Ñ–µ–π–∫–æ–≤–æ–≥–æ
        print(f"[URL-FALLBACK] No mapping found for title: {title}")
        return ""

    def _check_direct_l1_sources(self, ingredient: str, synonyms: Optional[List[str]], ai_model: Optional[str]) -> Optional[Dict[str, Any]]:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –ø—Ä—è–º—ñ L1 –¥–∂–µ—Ä–µ–ª–∞ (NIH, EFSA, FDA)"""
        try:
            print(f"[L1-DIRECT] Checking direct L1 sources for {ingredient}...")

            l1_urls = [
                f"https://ods.od.nih.gov/search?q={ingredient}",
                f"https://efsa.europa.eu/search?q={ingredient}",
                f"https://fda.gov/search?q={ingredient}"
            ]

            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó, –ø–æ–∫–∏ —â–æ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ None
            # –í –º–∞–π–±—É—Ç–Ω—å–æ–º—É —Ç—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ —Ä–µ–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ –ø–æ L1 —Å–∞–π—Ç–∞—Ö
            print(f"[L1-DIRECT] Direct L1 search not implemented yet")
            return None

        except Exception as e:
            print(f"[L1-DIRECT-ERROR] Error checking L1 sources: {e}")
            return None

    def _check_l2_sources(self, ingredient: str, synonyms: Optional[List[str]], ai_model: Optional[str]) -> Optional[Dict[str, Any]]:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î L2 –¥–∂–µ—Ä–µ–ª–∞ (Nature, ScienceDirect)"""
        try:
            print(f"[L2-ACADEMIC] Checking L2 academic sources for {ingredient}...")

            l2_urls = [
                f"https://nature.com/search?q={ingredient}",
                f"https://sciencedirect.com/search?q={ingredient}"
            ]

            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó, –ø–æ–∫–∏ —â–æ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ None
            # –í –º–∞–π–±—É—Ç–Ω—å–æ–º—É —Ç—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ —Ä–µ–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ –ø–æ L2 —Å–∞–π—Ç–∞—Ö
            print(f"[L2-ACADEMIC] L2 academic search not implemented yet")
            return None

        except Exception as e:
            print(f"[L2-ACADEMIC-ERROR] Error checking L2 sources: {e}")
            return None

    def _filter_google_results_by_l1_l4(self, google_result: Dict[str, Any], ingredient: str) -> Optional[Dict[str, Any]]:
        """–§—ñ–ª—å—Ç—Ä—É—î Gemini Google Search —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞ L1-L4 –¥–∂–µ—Ä–µ–ª–∞–º–∏"""
        try:
            print(f"[GEMINI-L1L4-FILTER] Filtering Gemini results by L1-L4 sources...")

            if not google_result or not google_result.get('sources'):
                print(f"[GEMINI-L1L4-FILTER] No sources to filter")
                return None

            # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ L1-L4 –¥–æ–º–µ–Ω—ñ–≤ –∑ source_policy
            l1_l4_domains = []
            try:
                # L1 domains
                l1_l4_domains.extend([
                    'nih.gov', 'ncbi.nlm.nih.gov', 'pubmed.ncbi.nlm.nih.gov',
                    'efsa.europa.eu', 'fda.gov', 'ods.od.nih.gov',
                    'scholar.google.com', 'wikipedia.org'
                ])
                # L2 domains
                l1_l4_domains.extend([
                    'nature.com', 'science.org', 'sciencedirect.com',
                    'examine.com', 'consumerlab.com'
                ])
                # L3 domains –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –ø—ñ–∑–Ω—ñ—à–µ

            except Exception as e:
                print(f"[GEMINI-L1L4-FILTER] Error getting L1-L4 domains: {e}")
                return None

            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ sources –∑–∞ L1-L4 –¥–æ–º–µ–Ω–∞–º–∏
            filtered_sources = []
            original_sources = google_result.get('sources', [])

            for source in original_sources:
                source_url = source.get('url', '')
                source_title = source.get('title', '')

                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ URL –º—ñ—Å—Ç–∏—Ç—å L1-L4 –¥–æ–º–µ–Ω–∏
                is_l1_l4 = False
                for domain in l1_l4_domains:
                    if domain in source_url.lower():
                        is_l1_l4 = True
                        print(f"[GEMINI-L1L4-FILTER] Accepted L1-L4 source: {domain} in {source_url}")
                        break

                if is_l1_l4:
                    filtered_sources.append(source)
                else:
                    print(f"[GEMINI-L1L4-FILTER] Rejected non-L1L4 source: {source_url}")

            if not filtered_sources:
                print(f"[GEMINI-L1L4-FILTER] No L1-L4 sources found in Gemini results")
                return None

            # –°—Ç–≤–æ—Ä—é—î–º–æ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            filtered_result = google_result.copy()
            filtered_result['sources'] = filtered_sources

            print(f"[GEMINI-L1L4-FILTER] Filtered {len(original_sources)} -> {len(filtered_sources)} L1-L4 sources")

            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç–∞–±–ª–∏—Ü—é
            return self._convert_google_result_to_table_ukrainian(filtered_result, ingredient)

        except Exception as e:
            print(f"[GEMINI-L1L4-FILTER-ERROR] Error filtering Gemini results: {e}")
            return None

    def _calculate_completion_stats(self, table_result: Dict[str, Any]) -> Dict[str, Any]:
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–ø–æ–≤–Ω–µ–Ω–æ—Å—Ç—ñ —Ç–∞–±–ª–∏—Ü—ñ"""
        try:
            total_fields = 5
            completed_fields = 0

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–Ω–µ –ø–æ–ª–µ
            if table_result.get('nazva_ukr_orig') and not '–Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞' in table_result['nazva_ukr_orig']:
                completed_fields += 1

            if table_result.get('dzherelo_syrovyny'):
                completed_fields += 1

            if table_result.get('aktyvni_spoluky') and len(table_result['aktyvni_spoluky']) > 0:
                completed_fields += 1

            if table_result.get('dobova_norma'):
                completed_fields += 1

            if table_result.get('dzherela_tsytaty') and len(table_result['dzherela_tsytaty']) > 0:
                completed_fields += 1

            percentage = (completed_fields / total_fields) * 100

            return {
                'total_fields': total_fields,
                'completed_fields': completed_fields,
                'percentage': percentage,
                'missing_fields': total_fields - completed_fields
            }

        except Exception as e:
            print(f"[COMPLETION-STATS-ERROR] Error calculating completion: {e}")
            return {'total_fields': 5, 'completed_fields': 0, 'percentage': 0.0, 'missing_fields': 5}


# Global instance
experimental_table_extractor = ExperimentalTableExtractor()