#!/usr/bin/env python3
"""
üîÑ MAIN PIPELINE - DLSD Evidence Collector
–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–π STAGES-BASED –ø–∞–π–ø–ª–∞–π–Ω —è–∫–∏–π –ø—Ä–∞—Ü—é–≤–∞–≤:
- Stage 0: Normalizer (normalizer.py)
- Stage 1: Searcher (searcher.py)
- Stage 2: Judge (judge.py)
- Stage 4: Table Extractor (table_extractor_experimental.py)
- –ê–¥–∞–ø—Ç–æ–≤–∞–Ω–∏–π –¥–ª—è A-G —Å—Ç–æ–≤–ø—á–∏–∫—ñ–≤ –∑–≥—ñ–¥–Ω–æ –∑ CLAUDE.md
"""
import sys
import os
import json
import time
from typing import Dict, List, Any, Optional
from datetime import datetime

# Fix Windows encoding issues
if sys.platform == "win32":
    import codecs
    try:
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)
    except AttributeError:
        pass

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from processes.normalizer import normalizer
from processes.searcher import searcher
from processes.judge import judge
from processes.table_extractor_experimental import ExperimentalTableExtractor
from modules.sheets_writer import sheets_writer
from config import Config

class EvidencePipeline:
    """–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–π stages-based –ø–∞–π–ø–ª–∞–π–Ω –∑ A-G —Ñ–æ—Ä–º–∞—Ç–æ–º –≤–∏–≤–æ–¥—É"""

    def __init__(self):
        self.pipeline_stats = {
            'total_processed': 0,
            'normalizer_success': 0,
            'searcher_success': 0,
            'judge_success': 0,
            'extraction_success': 0,
            'sheets_written': 0
        }

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ table extractor
        self.table_extractor = ExperimentalTableExtractor()

    def initialize_sheets(self) -> bool:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î Google Sheets –¥–ª—è –∑–∞–ø–∏—Å—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤"""
        try:
            # Sheets Writer –≤–∂–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –ø—Ä–∏ —ñ–º–ø–æ—Ä—Ç—ñ
            return sheets_writer.service is not None
        except Exception as e:
            print(f"[ERROR] Failed to initialize sheets: {e}")
            return False

    def process_ingredient(self, ingredient: str, synonyms: Optional[List[str]] = None,
                          existing_links: Optional[List[str]] = None, ai_model: str = None) -> Dict[str, Any]:
        """
        –í–Ü–î–ù–û–í–õ–ï–ù–ò–ô STAGES-BASED –ü–ê–ô–ü–õ–ê–ô–ù —è–∫–∏–π –ø—Ä–∞—Ü—é–≤–∞–≤ —Ä–∞–Ω—ñ—à–µ

        Args:
            ingredient: –ù–∞–∑–≤–∞ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∞
            synonyms: –°–ø–∏—Å–æ–∫ —Å–∏–Ω–æ–Ω—ñ–º—ñ–≤
            existing_links: –Ü—Å–Ω—É—é—á—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑—ñ —Å—Ç–æ–≤–ø—á–∏–∫–∞ G
            ai_model: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞ AI –º–æ–¥–µ–ª—å –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è Google Sheets A-G —Å—Ç–æ–≤–ø—á–∏–∫—ñ–≤
        """
        start_time = time.time()
        print(f"[STAGES-PIPELINE] Processing: {ingredient}")

        try:
            # STAGE 0: NORMALIZER - –ø–µ—Ä–µ–∫–ª–∞–¥ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
            print(f"[STAGE 0] Normalizing {ingredient}...")
            normalized_data = normalizer.normalize(ingredient, synonyms)
            if normalized_data:
                self.pipeline_stats['normalizer_success'] += 1
                print(f"[STAGE 0 OK] Normalized: {normalized_data.get('taxon', {}).get('uk', ingredient)}")
            else:
                print(f"[STAGE 0 FALLBACK] Using original ingredient name")
                normalized_data = {"ingredient": ingredient, "class": "unknown", "taxon": {"uk": ingredient, "lat": ingredient}}

            # STAGE 1: SEARCHER - –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø–æ—à—É–∫–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ —Ç–∞ –∑–±—ñ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ L1-L4
            print(f"[STAGE 1] Searching for candidates...")
            search_result = searcher.search_ingredient(normalized_data, synonyms)
            if search_result and search_result.get('candidates'):
                self.pipeline_stats['searcher_success'] += 1
                candidates = search_result['candidates']
                print(f"[STAGE 1 OK] Found {len(candidates)} candidate sources")
            else:
                print(f"[STAGE 1 WARNING] No candidates found")
                candidates = []

            # STAGE 2: JUDGE - —Ä–∞–Ω–≥—É–≤–∞–Ω–Ω—è –¥–∂–µ—Ä–µ–ª –∑–∞ L1-L4 –ø–æ–ª—ñ—Ç–∏–∫–æ—é
            print(f"[STAGE 2] Judging {len(candidates)} candidates...")
            judge_result = judge.judge_candidates(candidates)
            if judge_result and judge_result.get('accepted'):
                self.pipeline_stats['judge_success'] += 1
                accepted_sources = judge_result['accepted']
                print(f"[STAGE 2 OK] {len(accepted_sources)} sources accepted")
            else:
                print(f"[STAGE 2 WARNING] No sources accepted")
                accepted_sources = []

            # STAGE 4: TABLE EXTRACTOR - –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö —É —Ç–∞–±–ª–∏—á–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç
            print(f"[STAGE 4] Extracting table data from {len(accepted_sources)} sources...")
            table_result = self.table_extractor.extract_for_table_with_separate_bcd_cycles(
                normalized_data,
                accepted_sources,
                synonyms=synonyms,
                existing_links=existing_links,
                ai_model=ai_model
            )

            if table_result:
                self.pipeline_stats['extraction_success'] += 1
                print(f"[STAGE 4 OK] Table data extracted successfully")
            else:
                print(f"[STAGE 4 WARNING] Table extraction failed")
                table_result = {}

            # –ê–î–ê–ü–¢–ê–¶–Ü–Ø –î–û A-G –§–û–†–ú–ê–¢–£ - –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç table_extractor —É A-G —Å—Ç–æ–≤–ø—á–∏–∫–∏
            print(f"[DEBUG] Table extractor returned: {list(table_result.keys()) if table_result else 'None'}")
            if table_result:
                for key, value in table_result.items():
                    print(f"[DEBUG] {key}: {value}")
            result = self._convert_table_result_to_ag_format(ingredient, normalized_data, table_result)

            # –ó–ê–ü–ò–° –í GOOGLE SHEETS
            print(f"[SHEETS] Writing to Google Sheets...")
            try:
                table_data = {
                    'ingredient': ingredient,
                    'A_nazva_ukrainska': result['A_nazva_ukrainska'],
                    'B_dzherelo_otrymannya': result['B_dzherelo_otrymannya'],
                    'C_aktyvni_spoluky': result['C_aktyvni_spoluky'],
                    'D_dozuvannya': result['D_dozuvannya'],
                    'E_riven_dokaziv': result['E_riven_dokaziv'],
                    'F_tsytaty': result['F_tsytaty'],
                    'G_dzherela': result['G_dzherela']
                }

                sheets_written = sheets_writer.write_table_result(table_data)
                if sheets_written:
                    self.pipeline_stats['sheets_written'] += 1
                    print(f"[SHEETS OK] Written to Google Sheets")
                else:
                    print(f"[SHEETS WARNING] Failed to write to Google Sheets")
            except Exception as e:
                print(f"[SHEETS ERROR] {e}")

            # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
            processing_time = round(time.time() - start_time, 1)
            self.pipeline_stats['total_processed'] += 1

            has_ukrainian = bool(result['A_nazva_ukrainska'])
            has_source = bool(result['B_dzherelo_otrymannya'])
            has_compounds = bool(result['C_aktyvni_spoluky'])
            has_dosage = bool(result['D_dozuvannya'])
            has_citations = bool(result['F_tsytaty'])

            print(f"[STATS] Time: {processing_time}s | UA: {'‚úì' if has_ukrainian else '‚úó'} | Source: {'‚úì' if has_source else '‚úó'} | Compounds: {'‚úì' if has_compounds else '‚úó'} | Dose: {'‚úì' if has_dosage else '‚úó'} | Citations: {'‚úì' if has_citations else '‚úó'} | Level: {result['E_riven_dokaziv']}")

            return result

        except Exception as e:
            print(f"[STAGES-PIPELINE ERROR] Failed for {ingredient}: {e}")
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –±–∞–∑–æ–≤–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑ –ø–æ–º–∏–ª–∫–æ—é
            return self._create_fallback_ag_result(ingredient, str(e))

    def _convert_table_result_to_ag_format(self, ingredient: str, normalized_data: Dict[str, Any], table_result: Dict[str, Any]) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç table_extractor —É A-G —Ñ–æ—Ä–º–∞—Ç"""

        # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –Ω–∞–∑–≤: 1) table_result –∑ AI, 2) normalized_data, 3) fallback
        a_nazva = ""

        # 1. –°–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞–∑–≤—É –∑ table_result (AI –ø–µ—Ä–µ–∫–ª–∞–¥)
        if table_result.get('nazva_ukr_orig') and table_result['nazva_ukr_orig'].strip():
            a_nazva = table_result['nazva_ukr_orig']

        # 2. –Ø–∫—â–æ –Ω–µ–º–∞—î - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ normalized_data
        elif normalized_data.get('taxon', {}).get('uk'):
            uk_name = normalized_data['taxon']['uk']
            if uk_name != ingredient and '(' not in uk_name:
                a_nazva = f"{uk_name} ({ingredient})"

        # 3. Fallback - —Å—Ç–≤–æ—Ä—é—î–º–æ –±–∞–∑–æ–≤–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥ –¥–ª—è –≤—ñ–¥–æ–º–∏—Ö –∞–±—Ä–µ–≤—ñ–∞—Ç—É—Ä
        if not a_nazva:
            # –ü—Ä–æ—Å—Ç–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω–∏—Ö —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤
            translations = {
                "AHCC": "–ê–•–¶–¶ (AHCC)",
                "CoQ10": "–ö–æ–µ–Ω–∑–∏–º Q10 (CoQ10)",
                "ATP": "–ê–¢–§ (ATP)",
                "DNA": "–î–ù–ö (DNA)",
                "RNA": "–†–ù–ö (RNA)",
                "EPA": "–ï–ü–ö (EPA)",
                "DHA": "–î–ì–ö (DHA)"
            }
            a_nazva = translations.get(ingredient, f"{ingredient} ({ingredient})")

        result = {
            'A_nazva_ukrainska': a_nazva,  # –ó normalized_data –∞–±–æ table_result
            'B_dzherelo_otrymannya': table_result.get('dzherelo_syrovyny', ''),  # –î–∂–µ—Ä–µ–ª–æ —Å–∏—Ä–æ–≤–∏–Ω–∏
            'C_aktyvni_spoluky': self._format_compounds(table_result.get('aktyvni_spoluky', [])),  # –ê–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏
            'D_dozuvannya': table_result.get('dobova_norma', ''),  # –î–æ–∑—É–≤–∞–Ω–Ω—è
            'E_riven_dokaziv': self._determine_evidence_level(table_result),  # –†—ñ–≤–µ–Ω—å –¥–æ–∫–∞–∑—ñ–≤
            'F_tsytaty': self._format_citations(table_result.get('dzherela_tsytaty', [])),  # –¶–∏—Ç–∞—Ç–∏ –∑ URL
            'G_dzherela': ''  # –ü–æ—Ä–æ–∂–Ω—ñ–π, –æ—Å–∫—ñ–ª—å–∫–∏ URL —Ç–µ–ø–µ—Ä –≤ —Å—Ç–æ–≤–ø—á–∏–∫—É F
        }

        return result

    def _format_compounds(self, compounds: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç—É—î –∞–∫—Ç–∏–≤–Ω—ñ —Å–ø–æ–ª—É–∫–∏ —É —Å—Ç—Ä–æ–∫—É"""
        if not compounds:
            return ''

        # –Ø–∫—â–æ –≤–∂–µ —î —Å—Ç—Ä–æ–∫–∞, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î
        if isinstance(compounds, str):
            return compounds

        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Å–ø–∏—Å–æ–∫ —É —Å—Ç—Ä–æ–∫—É
        return ', '.join(compounds[:5])  # –ú–∞–∫—Å–∏–º—É–º 5 —Å–ø–æ–ª—É–∫

    def _format_citations(self, citations: List[Dict[str, Any]]) -> str:
        """–§–æ—Ä–º–∞—Ç—É—î —Ü–∏—Ç–∞—Ç–∏ –∑ URL —É —Å—Ç—Ä–æ–∫—É –¥–ª—è —Å—Ç–æ–≤–ø—á–∏–∫–∞ F"""
        if not citations:
            return ''

        # –Ø–∫—â–æ –≤–∂–µ —î —Å—Ç—Ä–æ–∫–∞, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î
        if isinstance(citations, str):
            return citations

        formatted = []
        for citation in citations[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 —Ü–∏—Ç–∞—Ç–∏
            if isinstance(citation, dict):
                quote = citation.get('quote', '')
                url = citation.get('url', '')

                if quote and len(quote) > 10:  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Ü–∏—Ç–∞—Ç–∏
                    # –§–æ—Ä–º–∞—Ç: "—Ü–∏—Ç–∞—Ç–∞ (URL)"
                    citation_text = quote[:100]  # –û–±—Ä—ñ–∑–∞—î–º–æ –¥–æ–≤–≥—ñ —Ü–∏—Ç–∞—Ç–∏
                    if url:
                        citation_text += f" ({url})"
                    formatted.append(citation_text)
            elif isinstance(citation, str):
                formatted.append(citation[:100])

        return '; '.join(formatted)

    def _format_sources(self, sources: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç—É—î –¥–∂–µ—Ä–µ–ª–∞ —É —Å—Ç—Ä–æ–∫—É –¥–ª—è —Å—Ç–æ–≤–ø—á–∏–∫–∞ G"""
        if not sources:
            return ''

        # –Ø–∫—â–æ –≤–∂–µ —î —Å—Ç—Ä–æ–∫–∞, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î
        if isinstance(sources, str):
            return sources

        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –≤–∞–ª—ñ–¥–Ω—ñ URL
        valid_urls = []
        for source in sources[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 URL
            if isinstance(source, str) and ('http' in source or 'doi:' in source):
                valid_urls.append(source)

        return '; '.join(valid_urls)

    def _extract_urls_from_citations(self, citations: List[Dict[str, Any]]) -> str:
        """–í–∏—Ç—è–≥—É—î URL –∑ —Ü–∏—Ç–∞—Ç –¥–ª—è —Å—Ç–æ–≤–ø—á–∏–∫–∞ G"""
        if not citations:
            return ''

        # –Ø–∫—â–æ –≤–∂–µ —î —Å—Ç—Ä–æ–∫–∞, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î
        if isinstance(citations, str):
            return citations

        # –í–∏—Ç—è–≥—É—î–º–æ URL –∑ —Ü–∏—Ç–∞—Ç
        urls = []
        for citation in citations[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 URL
            if isinstance(citation, dict):
                url = citation.get('url', '')
                if url and 'http' in url:
                    urls.append(url)

        return '; '.join(urls)

    def _determine_evidence_level(self, table_result: Dict[str, Any]) -> str:
        """–í–∏–∑–Ω–∞—á–∞—î —Ä—ñ–≤–µ–Ω—å –¥–æ–∫–∞–∑—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∂–µ—Ä–µ–ª"""
        # –í–∏—Ç—è–≥—É—î–º–æ URL –∑ —Ü–∏—Ç–∞—Ç –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä—ñ–≤–Ω—è
        citations = table_result.get('dzherela_tsytaty', [])
        sources = []

        for citation in citations:
            if isinstance(citation, dict):
                url = citation.get('url', '')
                if url:
                    sources.append(url)

        # –Ø–∫—â–æ –Ω–µ–º–∞—î URL –≤ —Ü–∏—Ç–∞—Ç–∞—Ö, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ sources
        if not sources:
            sources = table_result.get('sources', [])

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞–π–∫—Ä–∞—â—ñ –¥–æ–º–µ–Ω–∏ –¥–ª—è L1-L4
        for source in sources:
            if isinstance(source, str):
                if any(domain in source.lower() for domain in ['pubmed', 'ncbi.nlm.nih.gov', 'efsa.europa.eu']):
                    return 'L1'
                elif any(domain in source.lower() for domain in ['nature.com', 'sciencedirect.com']):
                    return 'L2'
                elif any(domain in source.lower() for domain in ['examine.com', 'wiley.com']):
                    return 'L3'

        return 'L4'  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º

    def _create_fallback_ag_result(self, ingredient: str, error_msg: str) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä—é—î fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É A-G —Ñ–æ—Ä–º–∞—Ç—ñ"""
        return {
            'A_nazva_ukrainska': f"{ingredient} ({ingredient})",
            'B_dzherelo_otrymannya': '',
            'C_aktyvni_spoluky': '',
            'D_dozuvannya': '',
            'E_riven_dokaziv': 'L4',
            'F_tsytaty': f'–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏: {error_msg[:100]}',
            'G_dzherela': ''
        }

    def get_pipeline_stats(self) -> Dict[str, int]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        return self.pipeline_stats.copy()

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫–∑–µ–º–ø–ª—è—Ä
pipeline = EvidencePipeline()